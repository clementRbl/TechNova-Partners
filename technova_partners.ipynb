{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4086c666",
   "metadata": {},
   "source": [
    "# TechNova Partners - Analyse du Churn RH\n",
    "\n",
    "**Projet :** Identification des causes de démission et modélisation prédictive  \n",
    "**Client :** TechNova Partners (ESN spécialisée en transformation digitale)\n",
    "\n",
    "---\n",
    "\n",
    "## Contexte du Projet\n",
    "\n",
    "TechNova Partners fait face à un turnover élevé. L'objectif est de :\n",
    "\n",
    "1. **Analyser** les données RH pour identifier les différences entre employés partis et restés\n",
    "2. **Construire** un modèle de classification pour prédire les démissions\n",
    "3. **Extraire** les causes potentielles via l'interprétation du modèle (SHAP)\n",
    "\n",
    "**Sources de données :**\n",
    "\n",
    "- `data/extrait_sirh.csv` - Informations RH (âge, salaire, poste, ancienneté...)\n",
    "- `data/extrait_eval.csv` - Évaluations de performance\n",
    "- `data/extrait_sondage.csv` - Sondage employés + **variable cible**\n",
    "\n",
    "---\n",
    "\n",
    "## Structure du Notebook\n",
    "\n",
    "**Partie 1 : Exploration des Données**\n",
    "\n",
    "- Chargement et compréhension des fichiers\n",
    "- Fusion et création du dataset central\n",
    "- Analyse exploratoire et visualisations\n",
    "\n",
    "**Partie 2 : Feature Engineering**\n",
    "\n",
    "- Préparation des features (X)\n",
    "- Encodage des variables catégorielles\n",
    "- Gestion des corrélations\n",
    "\n",
    "**Partie 3 : Modélisation Baseline**\n",
    "\n",
    "- Modèle Dummy (référence)\n",
    "- Modèle linéaire\n",
    "- Modèle non-linéaire (arbre)\n",
    "\n",
    "**Partie 4 : Gestion du Déséquilibre**\n",
    "\n",
    "- Stratification\n",
    "- Class weights / Undersampling / Oversampling (SMOTE)\n",
    "- Validation croisée stratifiée\n",
    "\n",
    "**Partie 5 : Optimisation et Interpretation**\n",
    "\n",
    "- Fine-tuning des hyperparamètres (GridSearchCV)\n",
    "- Feature importance globale (SHAP Beeswarm)\n",
    "- Feature importance locale (SHAP Waterfall)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1a5ad0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Importation des librairies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dba98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306b0aaf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Chargement des données\n",
    "\n",
    "Chargement des 3 fichiers CSV et examen de structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a584d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sirh = pd.read_csv(\"data/extrait_sirh.csv\")\n",
    "df_eval = pd.read_csv(\"data/extrait_eval.csv\")\n",
    "df_sondage = pd.read_csv(\"data/extrait_sondage.csv\")\n",
    "\n",
    "print(f\"Fichier SIRH : {df_sirh.shape[0]} lignes, {df_sirh.shape[1]} colonnes\")\n",
    "print(f\"Fichier Évaluations : {df_eval.shape[0]} lignes, {df_eval.shape[1]} colonnes\")\n",
    "print(f\"Fichier Sondage : {df_sondage.shape[0]} lignes, {df_sondage.shape[1]} colonnes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7859bfec",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Exploration initiale de chaque fichier\n",
    "\n",
    "Avant de fusionner, comprenons le contenu et la structure de chaque fichier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0cd45c",
   "metadata": {},
   "source": [
    "### 3.1 Fichier SIRH (extrait_sirh.csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52f0021",
   "metadata": {},
   "source": [
    "#### Aperçu des premières lignes\n",
    "\n",
    "Visualisons les premières lignes pour comprendre la structure et le contenu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dff1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sirh.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1555732",
   "metadata": {},
   "source": [
    "#### Structure et types de données\n",
    "\n",
    "Analysons les types de colonnes, la mémoire utilisée et les valeurs non-nulles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0529cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sirh.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a461b88d",
   "metadata": {},
   "source": [
    "#### Statistiques descriptives\n",
    "\n",
    "Calculons les statistiques de base (moyenne, écart-type, min, max, quartiles) pour les variables numériques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8b7892",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sirh.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5ab8be",
   "metadata": {},
   "source": [
    "#### Analyse des variables catégorielles\n",
    "\n",
    "Examinons les valeurs uniques et leur fréquence pour chaque variable catégorielle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf3e75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valeurs uniques des colonnes catégorielles SIRH :\")\n",
    "for col in df_sirh.select_dtypes(include=\"object\").columns:\n",
    "    print(f\"\\n{col}: {df_sirh[col].nunique()} valeurs uniques\")\n",
    "    print(df_sirh[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79318fd",
   "metadata": {},
   "source": [
    "### 3.2 Fichier Évaluations (extrait_eval.csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11411bc9",
   "metadata": {},
   "source": [
    "#### Aperçu des premières lignes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771a93d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6a667e",
   "metadata": {},
   "source": [
    "#### Structure et types de données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16286df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c928a9b5",
   "metadata": {},
   "source": [
    "#### Statistiques descriptives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837aef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistiques descriptives Evaluations (variables numeriques) :\")\n",
    "df_eval.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca31359",
   "metadata": {},
   "source": [
    "#### Analyse des variables catégorielles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d90ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valeurs uniques des colonnes catégorielles\n",
    "print(\"Valeurs uniques des colonnes catégorielles Evaluations :\")\n",
    "for col in df_eval.select_dtypes(include=\"object\").columns:\n",
    "    print(f\"\\n{col}: {df_eval[col].nunique()} valeurs uniques\")\n",
    "    print(df_eval[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d33eff",
   "metadata": {},
   "source": [
    "### 3.3 Fichier Sondage (extrait_sondage.csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400e8e4d",
   "metadata": {},
   "source": [
    "#### Aperçu des premières lignes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b17c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sondage.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c403c29a",
   "metadata": {},
   "source": [
    "#### Structure et types de données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d7ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sondage.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5579a95a",
   "metadata": {},
   "source": [
    "#### Statistiques descriptives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb217995",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sondage.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a13eaf",
   "metadata": {},
   "source": [
    "#### Analyse des variables catégorielles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b9cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_sondage.select_dtypes(include=\"object\").columns:\n",
    "    print(f\"\\n{col}: {df_sondage[col].nunique()} valeurs uniques\")\n",
    "    print(df_sondage[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8635855",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Identification des clés de jointure\n",
    "\n",
    "Pour fusionner les 3 fichiers, nous devons identifier les colonnes qui permettent de faire le lien entre eux.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a67bf0e",
   "metadata": {},
   "source": [
    "#### Analyse des colonnes identifiantes\n",
    "\n",
    "Examinons les colonnes qui nous permettront de faire les jointures entre fichiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42d5f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Colonnes SIRH :\")\n",
    "print(df_sirh.columns.tolist())\n",
    "print(\n",
    "    f\"\\nClé potentielle 'id_employee' : {df_sirh['id_employee'].nunique()} valeurs uniques sur {len(df_sirh)} lignes\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"\\nColonnes Évaluations :\")\n",
    "print(df_eval.columns.tolist())\n",
    "print(\n",
    "    f\"\\nClé potentielle 'eval_number' : {df_eval['eval_number'].nunique()} valeurs uniques sur {len(df_eval)} lignes\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"\\nColonnes Sondage :\")\n",
    "print(df_sondage.columns.tolist())\n",
    "print(\n",
    "    f\"\\nClé potentielle 'code_sondage' : {df_sondage['code_sondage'].nunique()} valeurs uniques sur {len(df_sondage)} lignes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9872483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysons le format des clés pour comprendre comment les relier\n",
    "print(\"Exemples de clés :\")\n",
    "print(f\"\\nSIRH - id_employee (premiers) : {df_sirh['id_employee'].head(10).tolist()}\")\n",
    "print(f\"\\nEval - eval_number (premiers) : {df_eval['eval_number'].head(10).tolist()}\")\n",
    "print(\n",
    "    f\"\\nSondage - code_sondage (premiers) : {df_sondage['code_sondage'].head(10).tolist()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a199f363",
   "metadata": {},
   "source": [
    "#### Analyse du format des clés\n",
    "\n",
    "Regardons de plus près comment sont structurées ces clés.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a85a308",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Comparaison du nombre de lignes :\")\n",
    "print(f\"  - SIRH : {len(df_sirh)} lignes\")\n",
    "print(f\"  - Évaluations : {len(df_eval)} lignes\")\n",
    "print(f\"  - Sondage : {len(df_sondage)} lignes\")\n",
    "\n",
    "# Si tous les fichiers ont le même nombre de lignes,\n",
    "# ils correspondent probablement aux mêmes employés"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2828de03",
   "metadata": {},
   "source": [
    "#### Vérification de la cohérence des données\n",
    "\n",
    "Comparons le nombre de lignes pour détecter d'éventuels problèmes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b40ef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recherche de colonnes communes entre les fichiers :\\n\")\n",
    "\n",
    "sirh_cols = set(df_sirh.columns)\n",
    "eval_cols = set(df_eval.columns)\n",
    "sondage_cols = set(df_sondage.columns)\n",
    "\n",
    "print(f\"SIRH ∩ Évaluations : {sirh_cols.intersection(eval_cols)}\")\n",
    "print(f\"SIRH ∩ Sondage : {sirh_cols.intersection(sondage_cols)}\")\n",
    "print(f\"Évaluations ∩ Sondage : {eval_cols.intersection(sondage_cols)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if not sirh_cols.intersection(eval_cols) and not sirh_cols.intersection(sondage_cols):\n",
    "    print(\"Résultat : Aucune colonne commune détectée\")\n",
    "    print(\"Les 3 fichiers ont des colonnes strictement différentes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed2fdca",
   "metadata": {},
   "source": [
    "#### Comparaison visuelle des clés\n",
    "\n",
    "Analysons la structure des clés pour identifier leur correspondance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dcf30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Comparaison visuelle des premières valeurs de chaque clé :\\n\")\n",
    "\n",
    "comparison_df = pd.DataFrame(\n",
    "    {\n",
    "        \"id_employee\": df_sirh[\"id_employee\"].head(10),\n",
    "        \"eval_number\": df_eval[\"eval_number\"].head(10),\n",
    "        \"code_sondage\": df_sondage[\"code_sondage\"].head(10),\n",
    "    }\n",
    ")\n",
    "print(comparison_df)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Observation : Les 3 clés suivent un pattern cohérent\")\n",
    "print(\"  - id_employee : valeurs numériques (1, 2, 3...)\")\n",
    "print(\"  - eval_number : format 'E_X' où X correspond à id_employee\")\n",
    "print(\"  - code_sondage : même valeur que id_employee\")\n",
    "print(\"\\nConclusion : Les lignes sont alignées par leur position (index)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c9a3e0",
   "metadata": {},
   "source": [
    "#### Conclusion : Stratégie de fusion\n",
    "\n",
    "**Constat :**\n",
    "\n",
    "- Aucune colonne commune entre les 3 fichiers\n",
    "- Même nombre de lignes (1470) dans chaque fichier\n",
    "- Les clés suivent un pattern cohérent suggérant un alignement par index\n",
    "- Chaque fichier contient des informations complémentaires :\n",
    "  - SIRH → infos administratives (ancienneté, salaire, département...)\n",
    "  - Évaluations → métriques de performance (notes, satisfaction...)\n",
    "  - Sondage → perception des employés (stress, équilibre vie pro/perso...)\n",
    "\n",
    "**Stratégie retenue :**\n",
    "\n",
    "Concaténation horizontale par index avec `pd.concat([df_sirh, df_eval, df_sondage], axis=1)`\n",
    "\n",
    "**Justification :**\n",
    "\n",
    "- Les lignes sont déjà alignées (id_employee=1 ↔ eval_number=\"E_1\" ↔ code_sondage=1)\n",
    "- Pas besoin de jointure SQL complexe\n",
    "- Les colonnes identifiantes seront conservées pour traçabilité\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831a01f1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Fusion des données\n",
    "\n",
    "Création du DataFrame central en fusionnant les 3 sources par concaténation horizontale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b6ff60",
   "metadata": {},
   "source": [
    "#### Création du DataFrame central\n",
    "\n",
    "Fusion des 3 fichiers par concaténation horizontale (axis=1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b5a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.concat([df_sirh, df_eval, df_sondage], axis=1)\n",
    "\n",
    "print(\"DataFrame fusionné créé :\")\n",
    "print(f\"  - {df_merged.shape[0]} lignes\")\n",
    "print(f\"  - {df_merged.shape[1]} colonnes\")\n",
    "print(f\"\\nColonnes : {df_merged.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0834aa13",
   "metadata": {},
   "source": [
    "#### Gestion des colonnes dupliquées\n",
    "\n",
    "Vérification et suppression des éventuelles colonnes en double.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ec4637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification des colonnes dupliquées\n",
    "duplicated_cols = df_merged.columns[df_merged.columns.duplicated()].tolist()\n",
    "\n",
    "if duplicated_cols:\n",
    "    print(f\"{len(duplicated_cols)} colonne(s) dupliquée(s) détectée(s) :\")\n",
    "    for col in duplicated_cols:\n",
    "        print(f\"  - {col}\")\n",
    "\n",
    "    # Suppression des doublons (on garde la première occurrence)\n",
    "    df_merged = df_merged.loc[:, ~df_merged.columns.duplicated()]\n",
    "    print(\"\\nColonnes dupliquées supprimées\")\n",
    "    print(f\"Nouvelles dimensions : {df_merged.shape}\")\n",
    "else:\n",
    "    print(\"Aucune colonne dupliquée détectée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca5fca7",
   "metadata": {},
   "source": [
    "#### Aperçu du DataFrame central\n",
    "\n",
    "Visualisation des premières lignes du dataset fusionné.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b7fd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05128d7c",
   "metadata": {},
   "source": [
    "#### Structure du DataFrame fusionné\n",
    "\n",
    "Informations sur les types de données et la mémoire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a48f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367a229f",
   "metadata": {},
   "source": [
    "#### Analyse de la variable cible\n",
    "\n",
    "Distribution de `a_quitte_l_entreprise` - la variable à prédire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9daf702",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Variable cible - 'a_quitte_l_entreprise' :\")\n",
    "print(f\"Type : {df_merged['a_quitte_l_entreprise'].dtype}\")\n",
    "print(\"\\nDistribution :\")\n",
    "print(df_merged[\"a_quitte_l_entreprise\"].value_counts())\n",
    "print(\"\\nProportions (%) :\")\n",
    "print((df_merged[\"a_quitte_l_entreprise\"].value_counts(normalize=True) * 100).round(2))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Comptage\n",
    "df_merged[\"a_quitte_l_entreprise\"].value_counts().plot(\n",
    "    kind=\"bar\", ax=ax[0], color=[\"#2ecc71\", \"#e74c3c\"]\n",
    ")\n",
    "ax[0].set_title(\"Distribution de la variable cible\", fontsize=12, fontweight=\"bold\")\n",
    "ax[0].set_xlabel(\"A quitté l'entreprise\")\n",
    "ax[0].set_ylabel(\"Nombre d'employés\")\n",
    "ax[0].set_xticklabels([\"Non\", \"Oui\"], rotation=0)\n",
    "\n",
    "# Proportions\n",
    "df_merged[\"a_quitte_l_entreprise\"].value_counts().plot(\n",
    "    kind=\"pie\",\n",
    "    ax=ax[1],\n",
    "    autopct=\"%1.1f%%\",\n",
    "    colors=[\"#2ecc71\", \"#e74c3c\"],\n",
    "    labels=[\"Restés\", \"Partis\"],\n",
    ")\n",
    "ax[1].set_title(\"Proportions\", fontsize=12, fontweight=\"bold\")\n",
    "ax[1].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"OBSERVATION CRITIQUE : Déséquilibre des classes !\")\n",
    "print(\"     → À gérer en modélisation (stratification, class_weights, SMOTE)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab20774",
   "metadata": {},
   "source": [
    "#### Synthèse : Variable cible\n",
    "\n",
    "- **84% restés** vs **16% partis** → Ratio 5:1\n",
    "- Déséquilibre à gérer : stratification, class_weight, resampling\n",
    "- Accuracy insuffisante comme métrique (84% sans rien faire)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebb775a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Vue d'ensemble du dataset central\n",
    "\n",
    "Avant de comparer les employés partis vs restés, vérifions la qualité et la structure des données :\n",
    "\n",
    "- Valeurs manquantes\n",
    "- Types de colonnes (numériques vs catégorielles)\n",
    "- Colonnes identifiantes à exclure de l'analyse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db38ef20",
   "metadata": {},
   "source": [
    "#### Analyse des valeurs manquantes\n",
    "\n",
    "Vérifions s'il y a des données manquantes dans le dataset fusionné.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f54449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des valeurs manquantes\n",
    "print(\"Analyse des valeurs manquantes :\\n\")\n",
    "\n",
    "missing_values = df_merged.isnull().sum()\n",
    "missing_pct = (df_merged.isnull().sum() / len(df_merged) * 100).round(2)\n",
    "\n",
    "missing_df = pd.DataFrame(\n",
    "    {\"Valeurs manquantes\": missing_values, \"Pourcentage (%)\": missing_pct}\n",
    ")\n",
    "\n",
    "# Afficher seulement les colonnes avec des valeurs manquantes\n",
    "missing_with_values = missing_df[missing_df[\"Valeurs manquantes\"] > 0]\n",
    "\n",
    "if len(missing_with_values) > 0:\n",
    "    print(f\"{len(missing_with_values)} colonne(s) avec des valeurs manquantes :\")\n",
    "    print(missing_with_values.sort_values(\"Pourcentage (%)\", ascending=False))\n",
    "else:\n",
    "    print(\"Aucune valeur manquante dans le dataset !\")\n",
    "    print(f\"   → {df_merged.shape[0]} lignes × {df_merged.shape[1]} colonnes complètes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2018e0d7",
   "metadata": {},
   "source": [
    "#### Classification des colonnes par type\n",
    "\n",
    "Identifions les colonnes numériques et catégorielles pour orienter l'analyse exploratoire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f358b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification des colonnes par type :\\n\")\n",
    "\n",
    "# Colonnes identifiantes (à exclure de l'analyse)\n",
    "id_cols = [\"id_employee\", \"eval_number\", \"code_sondage\"]\n",
    "\n",
    "# Variable cible\n",
    "target_col = \"a_quitte_l_entreprise\"\n",
    "\n",
    "# Colonnes numériques (excluant les IDs)\n",
    "numeric_cols = df_merged.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "numeric_cols = [col for col in numeric_cols if col not in id_cols + [target_col]]\n",
    "\n",
    "# Colonnes catégorielles\n",
    "categorical_cols = df_merged.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "categorical_cols = [\n",
    "    col for col in categorical_cols if col not in id_cols + [target_col]\n",
    "]\n",
    "\n",
    "print(f\"Colonnes identifiantes ({len(id_cols)}) - À EXCLURE :\")\n",
    "print(f\"   {id_cols}\\n\")\n",
    "\n",
    "print(\"Variable cible :\")\n",
    "print(f\"   {target_col}\\n\")\n",
    "\n",
    "print(f\"Colonnes numériques ({len(numeric_cols)}) :\")\n",
    "print(f\"   {numeric_cols}\\n\")\n",
    "\n",
    "print(f\"Colonnes catégorielles ({len(categorical_cols)}) :\")\n",
    "print(f\"   {categorical_cols}\")\n",
    "\n",
    "print(f\"Total features analysables : {len(numeric_cols) + len(categorical_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3001d5e",
   "metadata": {},
   "source": [
    "#### Résumé structuré du dataset\n",
    "\n",
    "Tableau récapitulatif avec le type, les valeurs uniques et des exemples pour chaque colonne.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f860ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = []\n",
    "\n",
    "for col in df_merged.columns:\n",
    "    if col in id_cols:\n",
    "        category = \"Identifiant\"\n",
    "    elif col == target_col:\n",
    "        category = \"Cible\"\n",
    "    elif col in numeric_cols:\n",
    "        category = \"Numérique\"\n",
    "    else:\n",
    "        category = \"Catégorielle\"\n",
    "\n",
    "    summary_data.append(\n",
    "        {\n",
    "            \"Colonne\": col,\n",
    "            \"Catégorie\": category,\n",
    "            \"Type\": str(df_merged[col].dtype),\n",
    "            \"Valeurs uniques\": df_merged[col].nunique(),\n",
    "            \"Exemple\": str(df_merged[col].iloc[0])[:30],\n",
    "        }\n",
    "    )\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df6fd0b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Analyse exploratoire comparative : Partis vs Restés\n",
    "\n",
    "Objectif principal de cette section : **identifier les différences clés** entre les employés ayant quitté l'entreprise et ceux qui y sont restés.\n",
    "\n",
    "Nous utiliserons **Plotly** pour des graphiques interactifs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2343e5e7",
   "metadata": {},
   "source": [
    "#### Import de Plotly et préparation des données\n",
    "\n",
    "Configuration de Plotly et création d'une colonne lisible pour la variable cible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c74251",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[\"statut\"] = df_merged[\"a_quitte_l_entreprise\"].map(\n",
    "    {\"Oui\": \"Parti\", \"Non\": \"Resté\"}\n",
    ")\n",
    "\n",
    "colors = {\"Resté\": \"#2ecc71\", \"Parti\": \"#e74c3c\"}\n",
    "\n",
    "print(f\"   Distribution : {df_merged['statut'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356fc251",
   "metadata": {},
   "source": [
    "### 7.1 Analyse des variables numériques\n",
    "\n",
    "Comparons les **moyennes** des variables numériques entre les employés partis et restés avec un graphique unique et lisible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90941077",
   "metadata": {},
   "source": [
    "#### Observations : Variables numériques\n",
    "\n",
    "**Principales différences observées :**\n",
    "\n",
    "| Variable                      | Différence | Observation                          |\n",
    "| ----------------------------- | ---------- | ------------------------------------ |\n",
    "| `nombre_participation_pee`    | -37.6%     | Participation PEE plus faible        |\n",
    "| `annees_dans_le_poste_actuel` | -35.3%     | Ancienneté dans le poste plus faible |\n",
    "| `revenu_mensuel`              | -29.9%     | Salaire plus bas                     |\n",
    "| `distance_domicile_travail`   | +19.3%     | Distance plus grande                 |\n",
    "\n",
    "**Note :** Ce sont des observations descriptives. Le modèle confirmera l'importance réelle de chaque variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e78f13e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Synthèse Partie 1 : Analyse Exploratoire\n",
    "\n",
    "### Données fusionnées\n",
    "\n",
    "| Métrique           | Valeur                       |\n",
    "| ------------------ | ---------------------------- |\n",
    "| Employés           | 1470                         |\n",
    "| Variables          | 34 colonnes                  |\n",
    "| Sources            | SIRH + Évaluations + Sondage |\n",
    "| Valeurs manquantes | 0                            |\n",
    "\n",
    "### Variable cible : Déséquilibre critique\n",
    "\n",
    "| Classe | Effectif | Proportion |\n",
    "| ------ | -------- | ---------- |\n",
    "| Restés | 1233     | **84%**    |\n",
    "| Partis | 237      | **16%**    |\n",
    "\n",
    "→ **Ratio 5:1** : nécessite stratification + gestion du déséquilibre (class_weight, SMOTE)\n",
    "\n",
    "### Profil type de l'employé qui part\n",
    "\n",
    "**Variables numériques discriminantes :**\n",
    "\n",
    "| Variable                      | Écart vs Restés | Interprétation RH            |\n",
    "| ----------------------------- | --------------- | ---------------------------- |\n",
    "| `nombre_participation_pee`    | **-37.6%**      | Moins engagés financièrement |\n",
    "| `annees_dans_le_poste_actuel` | **-35.3%**      | Moins d'ancienneté poste     |\n",
    "| `revenu_mensuel`              | **-29.9%**      | Salaire plus bas             |\n",
    "| `distance_domicile_travail`   | **+19.3%**      | Trajet plus long             |\n",
    "\n",
    "**Variables catégorielles à risque :**\n",
    "\n",
    "| Variable                | Modalité à risque       | Taux de churn |\n",
    "| ----------------------- | ----------------------- | ------------- |\n",
    "| `poste`                 | Représentant Commercial | **39.8%**     |\n",
    "| `heure_supplementaires` | Oui                     | **30.5%**     |\n",
    "| `statut_marital`        | Célibataire             | **25.5%**     |\n",
    "| `frequence_deplacement` | Fréquent                | Taux élevé    |\n",
    "\n",
    "**Profils stables (faible churn) :**\n",
    "\n",
    "- Directeur Technique (2.5%), Manager (6.9%)\n",
    "- Pas d'heures sup (10.4%)\n",
    "- Mariés, ancienneté élevée\n",
    "\n",
    "### Insights métier pour les RH\n",
    "\n",
    "1. **Rémunération** : Les employés qui partent gagnent ~30% de moins → Revoir la politique salariale\n",
    "2. **Heures sup** : 30% de churn chez ceux qui en font → Surveiller la charge de travail\n",
    "3. **Mobilité** : Distance domicile-travail corrélée au départ → Télétravail comme levier\n",
    "4. **Engagement** : Faible participation PEE = signal d'alerte → Renforcer l'intéressement\n",
    "5. **Postes à risque** : Commerciaux = 40% de turnover → Actions ciblées\n",
    "\n",
    "### Variables retenues pour la modélisation\n",
    "\n",
    "**Numériques potentiellement prédictives :**\n",
    "\n",
    "- `revenu_mensuel`, `annees_dans_le_poste_actuel`, `nombre_participation_pee`\n",
    "- `distance_domicile_travail`, `satisfaction_*` (4 variables)\n",
    "\n",
    "**Catégorielles potentiellement prédictives :**\n",
    "\n",
    "- `heure_supplementaires`, `poste`, `statut_marital`, `frequence_deplacement`\n",
    "\n",
    "**⚠️ Attention** : Ces observations sont **descriptives**. Le modèle (puis SHAP) confirmera l'importance réelle de chaque variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cf5ceb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Partie 2 : Feature Engineering\n",
    "\n",
    "Dans cette partie, nous allons :\n",
    "\n",
    "1. **Nettoyer les donnees** : doublons, outliers, colonnes inutiles\n",
    "2. **Analyser les correlations** : matrice de Pearson, suppression des variables trop correlees\n",
    "3. **Encoder les variables categorielles** : OneHotEncoder pour les modeles\n",
    "4. **Creer X et y** : preparation finale pour la modelisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e376d06",
   "metadata": {},
   "source": [
    "## 8. Nettoyage des donnees\n",
    "\n",
    "### 8.1 Verification des doublons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ead847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"  Nombre de lignes dupliquees : {df_merged.duplicated().sum()}\")\n",
    "print(f\"\\nDoublons sur 'id_employee' : {df_merged['id_employee'].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57167662",
   "metadata": {},
   "source": [
    "### 8.2 Detection des outliers (methode IQR)\n",
    "\n",
    "**Qu'est-ce que la methode IQR (Interquartile Range) ?**\n",
    "\n",
    "L'IQR est une methode statistique robuste pour detecter les valeurs aberrantes :\n",
    "\n",
    "**Calcul des bornes :**\n",
    "\n",
    "- Borne inferieure = Q1 - 1.5 x IQR\n",
    "- Borne superieure = Q3 + 1.5 x IQR\n",
    "\n",
    "Toute valeur en dehors de ces bornes est consideree comme un **outlier**.\n",
    "\n",
    "**Pourquoi IQR plutot que Z-score ?**\n",
    "\n",
    "- IQR est base sur les **quartiles** (pas la moyenne)\n",
    "- Donc **insensible aux valeurs extremes** elles-memes\n",
    "- Plus adapte aux distributions non-normales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf2339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection des outliers avec la methode IQR (Interquartile Range)\n",
    "def detect_outliers_iqr(df, columns):\n",
    "    \"\"\"Detecte les outliers pour chaque colonne numerique avec la methode IQR.\"\"\"\n",
    "    outliers_summary = []\n",
    "\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        n_outliers = len(outliers)\n",
    "        pct_outliers = (n_outliers / len(df)) * 100\n",
    "\n",
    "        if n_outliers > 0:\n",
    "            outliers_summary.append(\n",
    "                {\n",
    "                    \"Variable\": col,\n",
    "                    \"Nb outliers\": n_outliers,\n",
    "                    \"% outliers\": round(pct_outliers, 1),\n",
    "                    \"Borne inf\": round(lower_bound, 2),\n",
    "                    \"Borne sup\": round(upper_bound, 2),\n",
    "                    \"Min reel\": round(df[col].min(), 2),\n",
    "                    \"Max reel\": round(df[col].max(), 2),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(outliers_summary)\n",
    "\n",
    "\n",
    "# Exclure les colonnes ID et la cible pour l'analyse des outliers\n",
    "cols_to_check = [\n",
    "    col\n",
    "    for col in numeric_cols\n",
    "    if col not in [\"id_employee\", \"eval_number\", \"code_sondage\"]\n",
    "]\n",
    "outliers_df = detect_outliers_iqr(df_merged, cols_to_check)\n",
    "\n",
    "print(f\"Variables avec outliers : {len(outliers_df)} / {len(cols_to_check)}\")\n",
    "print()\n",
    "if len(outliers_df) > 0:\n",
    "    display(outliers_df.sort_values(\"% outliers\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbe8f28",
   "metadata": {},
   "source": [
    "**Decision sur les outliers :**\n",
    "\n",
    "Les outliers detectes sont des valeurs coherentes dans un contexte RH :\n",
    "\n",
    "- **Revenus eleves** : salaires de cadres superieurs (jusqu'a 19 999 EUR)\n",
    "- **Anciennete elevee** : employes fideles (jusqu'a 40 ans)\n",
    "- **Formations** : 6 formations maximum, valeur plausible\n",
    "\n",
    "Ces valeurs ne sont pas des erreurs de saisie mais des cas legitimes. Nous les **conservons** car :\n",
    "\n",
    "1. Les modeles tree-based (Random Forest, XGBoost) gerent bien les outliers\n",
    "2. Ces profils extremes peuvent etre pertinents pour predire le churn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef98ecb",
   "metadata": {},
   "source": [
    "### 8.3 Identification des colonnes a supprimer\n",
    "\n",
    "**Pourquoi supprimer certaines colonnes ?**\n",
    "\n",
    "1. **Colonnes ID** (id_employee, eval_number, code_sondage)\n",
    "   - Ce sont des identifiants uniques (1, 2, 3...)\n",
    "   - Aucune valeur predictive : le modele ne peut pas apprendre que \"employe 42\" part plus souvent\n",
    "\n",
    "2. **Colonnes a variance nulle**\n",
    "   - Une colonne avec la **meme valeur pour tous** (ex: `nombre_heures_travailless = 80` pour tout le monde)\n",
    "   - Aucune information discriminante : impossible de differencier partis vs restes\n",
    "\n",
    "3. **Colonnes redondantes**\n",
    "   - `a_quitte_l_entreprise` et `statut` contiennent la meme information que notre cible\n",
    "   - Les garder = **data leakage** (le modele \"triche\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33bed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Colonnes actuelles du dataset :\")\n",
    "print(df_merged.columns.tolist())\n",
    "\n",
    "id_columns = [\"id_employee\", \"eval_number\", \"code_sondage\"]\n",
    "\n",
    "target_column = \"depart\"\n",
    "\n",
    "# Colonnes a variance nulle ou quasi-nulle\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"\\nVerification des colonnes a faible variance :\")\n",
    "for col in df_merged.columns:\n",
    "    unique_ratio = df_merged[col].nunique() / len(df_merged)\n",
    "    if df_merged[col].nunique() <= 2 and col != target_column:\n",
    "        print(\n",
    "            f\"  {col} : {df_merged[col].nunique()} valeurs uniques -> {df_merged[col].value_counts().to_dict()}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f78867",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = {\n",
    "    # Colonnes ID (pas de valeur predictive)\n",
    "    \"id_employee\": \"Identifiant unique - pas de valeur predictive\",\n",
    "    \"eval_number\": \"Identifiant evaluation - pas de valeur predictive\",\n",
    "    \"code_sondage\": \"Identifiant sondage - pas de valeur predictive\",\n",
    "    # Colonnes a variance nulle (meme valeur pour tous)\n",
    "    \"nombre_heures_travailless\": \"Variance nulle - toujours 80\",\n",
    "    \"nombre_employee_sous_responsabilite\": \"Variance nulle - toujours 1\",\n",
    "    \"ayant_enfants\": \"Variance nulle - toujours Y\",\n",
    "    # Colonne redondante avec la cible\n",
    "    \"a_quitte_l_entreprise\": 'Redondante avec \"statut\" (variable cible)',\n",
    "    \"statut\": 'Redondante - nous utiliserons \"depart\" comme cible binaire',\n",
    "}\n",
    "\n",
    "print(\"Colonnes a supprimer :\")\n",
    "for col, reason in columns_to_drop.items():\n",
    "    print(f\"  - {col}: {reason}\")\n",
    "\n",
    "print(f\"\\nTotal : {len(columns_to_drop)} colonnes a supprimer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21c2f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation du DataFrame nettoye\n",
    "df_clean = df_merged.copy()\n",
    "\n",
    "# Creation de la variable cible binaire 'depart' (0 = Reste, 1 = Churn)\n",
    "df_clean[\"depart\"] = (df_clean[\"statut\"] == \"Parti\").astype(int)\n",
    "\n",
    "# Suppression des colonnes identifiees\n",
    "df_clean = df_clean.drop(columns=list(columns_to_drop.keys()))\n",
    "\n",
    "print(f\"Dataset initial : {df_merged.shape[0]} lignes x {df_merged.shape[1]} colonnes\")\n",
    "print(f\"Dataset nettoye : {df_clean.shape[0]} lignes x {df_clean.shape[1]} colonnes\")\n",
    "print(f\"\\nColonnes restantes ({df_clean.shape[1]}) :\")\n",
    "print(df_clean.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb6926f",
   "metadata": {},
   "source": [
    "## 9. Analyse des correlations\n",
    "\n",
    "**Pourquoi analyser les correlations ?**\n",
    "\n",
    "1. **Identifier les relations lineaires** entre variables\n",
    "2. **Detecter la multicolinearite** : si 2 variables sont tres correlees (|r| > 0.7), elles apportent la meme information → on peut en supprimer une\n",
    "3. **Comprendre les liens avec la cible** : quelles variables sont les plus correlees avec le depart ?\n",
    "\n",
    "### 9.1 Matrice de correlation de Pearson\n",
    "\n",
    "**Pearson** mesure les correlations **lineaires** :\n",
    "\n",
    "- r = +1 : correlation positive parfaite\n",
    "- r = 0 : pas de correlation lineaire\n",
    "- r = -1 : correlation negative parfaite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbea7b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la matrice de correlation Pearson\n",
    "numeric_cols_clean = df_clean.select_dtypes(\n",
    "    include=[\"int64\", \"float64\"]\n",
    ").columns.tolist()\n",
    "print(f\"Variables numériques pour la corrélation : {len(numeric_cols_clean)}\")\n",
    "\n",
    "corr_matrix = df_clean[numeric_cols_clean].corr()\n",
    "\n",
    "# Visualisation avec matplotlib/seaborn\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    mask=mask,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"RdBu_r\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    ax=ax,\n",
    "    annot_kws={\"size\": 7},\n",
    "    cbar_kws={\"shrink\": 0.8},\n",
    ")\n",
    "plt.title(\"Matrice de Corrélation de Pearson\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Corrélations avec la cible\n",
    "print(\"\\nCORRÉLATIONS AVEC LA CIBLE 'depart' :\")\n",
    "print(\"-\" * 50)\n",
    "target_corr = corr_matrix[\"depart\"].drop(\"depart\").sort_values(key=abs, ascending=False)\n",
    "for var, corr_value in target_corr.items():\n",
    "    print(f\"   {var:40} : {corr_value:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c11216",
   "metadata": {},
   "source": [
    "### 9.2 Identification des correlations fortes (|r| > 0.7)\n",
    "\n",
    "**Pourquoi analyser les corrélations ?**\n",
    "\n",
    "Si deux variables sont **très corrélées** (ex: `revenu_mensuel` et `niveau_hierarchique_poste` à 0.95), elles apportent une information **partiellement redondante**.\n",
    "\n",
    "**Faut-il supprimer ces variables ?**\n",
    "\n",
    "⚠️ **Non !** Contrairement à une idée reçue, la **multicolinéarité n'est pas un problème** pour les modèles basés sur les arbres (Random Forest, LightGBM) que nous utilisons :\n",
    "\n",
    "- Ces modèles **n'utilisent pas de coefficients linéaires** → pas d'instabilité\n",
    "- Même avec 95% de corrélation, il reste **5% d'information unique** qui peut être utile\n",
    "- Supprimer une variable = **perte potentielle d'information prédictive**\n",
    "- **Pas de data leakage** : ces variables sont des caractéristiques RH légitimes\n",
    "\n",
    "**Conclusion :** On **conserve toutes les variables** et on laisse le modèle choisir les plus pertinentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e32385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification des paires de variables fortement correlees (|r| > 0.7)\n",
    "threshold = 0.7\n",
    "high_corr_pairs = []\n",
    "\n",
    "# Parcourir le triangle inferieur de la matrice (sans la diagonale)\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "            high_corr_pairs.append(\n",
    "                {\n",
    "                    \"Variable 1\": corr_matrix.columns[j],\n",
    "                    \"Variable 2\": corr_matrix.columns[i],\n",
    "                    \"Correlation\": round(corr_matrix.iloc[i, j], 3),\n",
    "                }\n",
    "            )\n",
    "\n",
    "high_corr_df = pd.DataFrame(high_corr_pairs).sort_values(\"Correlation\", ascending=False)\n",
    "print(f\"Paires de variables avec correlation |r| > {threshold} :\")\n",
    "print()\n",
    "display(high_corr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f2ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des corrélations avec la cible 'depart'\n",
    "# (pour information - toutes les variables sont conservées)\n",
    "\n",
    "print(\"Corrélation avec la cible 'depart' :\")\n",
    "target_corr = corr_matrix[\"depart\"].drop(\"depart\").abs().sort_values(ascending=False)\n",
    "print(target_corr.head(10))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"\\nAnalyse des paires fortement corrélées :\")\n",
    "for _, row in high_corr_df.iterrows():\n",
    "    var1, var2 = row[\"Variable 1\"], row[\"Variable 2\"]\n",
    "    corr1 = abs(corr_matrix.loc[\"depart\", var1])\n",
    "    corr2 = abs(corr_matrix.loc[\"depart\", var2])\n",
    "    print(\n",
    "        f\"\\n{var1} (|r| avec depart = {corr1:.3f}) vs {var2} (|r| avec depart = {corr2:.3f})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650d936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ DECISION : On garde TOUTES les variables corrélées\n",
    "# Justification métier :\n",
    "# - Même avec r=0.95, il reste 5% d'information unique potentiellement utile\n",
    "# - Les modèles à base d'arbres (RF, LightGBM) gèrent bien la multicolinéarité\n",
    "# - Pas de data leakage → pas de raison de supprimer\n",
    "\n",
    "print(\"✅ DÉCISION : Conservation de TOUTES les variables\")\n",
    "print()\n",
    "print(\"Variables fortement corrélées (|r| > 0.7) :\")\n",
    "for _, row in high_corr_df.iterrows():\n",
    "    print(\n",
    "        f\"   {row['Variable 1']} ↔ {row['Variable 2']} : r = {row['Correlation']:.2f}\"\n",
    "    )\n",
    "print()\n",
    "print(\"Justification :\")\n",
    "print(\"   - Les modèles à base d'arbres ne sont pas affectés par la multicolinéarité\")\n",
    "print(\"   - Même 5% d'information unique peut améliorer la prédiction\")\n",
    "print(\"   - Aucun data leakage détecté → variables RH légitimes\")\n",
    "print()\n",
    "print(f\"Dataset conservé : {df_clean.shape[0]} lignes x {df_clean.shape[1]} colonnes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d976f594",
   "metadata": {},
   "source": [
    "### 9.3 Pourquoi conserver les variables corrélées ?\n",
    "\n",
    "**Analyse des corrélations détectées (|r| > 0.7) :**\n",
    "\n",
    "| Variable 1                 | Variable 2                      | Corrélation |\n",
    "| -------------------------- | ------------------------------- | ----------- |\n",
    "| `revenu_mensuel`           | `niveau_hierarchique_poste`     | **0.95**    |\n",
    "| `annee_experience_totale`  | `niveau_hierarchique_poste`     | 0.78        |\n",
    "| `annees_dans_l_entreprise` | `annees_dans_le_poste_actuel`   | 0.76        |\n",
    "| `annees_dans_l_entreprise` | `annes_sous_responsable_actuel` | 0.77        |\n",
    "\n",
    "**Pourquoi NE PAS supprimer ces variables ?**\n",
    "\n",
    "1. **Modèles à base d'arbres** (RF, LightGBM) :\n",
    "   - Ne calculent pas de coefficients linéaires → pas d'instabilité\n",
    "   - Sélectionnent automatiquement les features les plus discriminantes\n",
    "   - La corrélation ne pose pas de problème d'estimation\n",
    "\n",
    "2. **Information résiduelle** :\n",
    "   - Même avec r=0.95, il reste **5% d'information unique**\n",
    "   - Cette information peut être utile pour certains profils d'employés\n",
    "   - Supprimer = risque de perdre du pouvoir prédictif\n",
    "\n",
    "3. **Interprétabilité SHAP** :\n",
    "   - SHAP répartit équitablement l'importance entre variables corrélées\n",
    "   - On peut voir les deux perspectives : salaire ET niveau hiérarchique\n",
    "\n",
    "4. **Pas de data leakage** :\n",
    "   - Ces variables sont des caractéristiques RH disponibles avant le départ\n",
    "   - Aucune fuite d'information future\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e769835",
   "metadata": {},
   "source": [
    "### 9.4 Conversion de `augementation_salaire_precedente` en numérique\n",
    "\n",
    "**Problème identifié :** La colonne `augementation_salaire_precedente` contient des valeurs comme \"11 %\", \"23 %\" qui sont stockées comme **texte (object)**.\n",
    "\n",
    "**Pourquoi convertir en numérique ?**\n",
    "\n",
    "| Approche                      | Nb features          | Compréhension modèle             | Risque overfitting |\n",
    "| ----------------------------- | -------------------- | -------------------------------- | ------------------ |\n",
    "| **Catégorielle (défaut)**     | 14 colonnes (OneHot) | ❌ Perd l'ordre et les distances | ⚠️ Plus élevé      |\n",
    "| **Numérique (best practice)** | 1 colonne            | ✅ 11% < 12% < 23%               | ✅ Réduit          |\n",
    "\n",
    "**Avantages de la conversion :**\n",
    "\n",
    "- Le modèle comprend que **23% > 11%** (relation ordinale préservée)\n",
    "- Le modèle comprend que **23% - 11% = 12 points** (distances préservées)\n",
    "- **1 feature** au lieu de **14** → moins de dimensions → moins d'overfitting\n",
    "- Coefficient plus **interprétable** : \"+1% d'augmentation = X% de risque de départ\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eda9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion de \"11 %\" -> 11.0 (valeur numerique)\n",
    "# On garde la valeur en pourcentage (11, 12, 23...) plutot qu'en decimal (0.11, 0.12...)\n",
    "\n",
    "print(\"Avant conversion :\")\n",
    "print(f\"  Type: {df_clean['augementation_salaire_precedente'].dtype}\")\n",
    "print(\n",
    "    f\"  Valeurs uniques: {df_clean['augementation_salaire_precedente'].unique()[:5]}...\"\n",
    ")\n",
    "\n",
    "# Suppression du \" %\" et conversion en float\n",
    "df_clean[\"augementation_salaire_precedente\"] = (\n",
    "    df_clean[\"augementation_salaire_precedente\"]\n",
    "    .str.replace(\" %\", \"\", regex=False)\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "print(\"\\nApres conversion :\")\n",
    "print(f\"  Type: {df_clean['augementation_salaire_precedente'].dtype}\")\n",
    "print(\n",
    "    f\"  Valeurs uniques: {sorted(df_clean['augementation_salaire_precedente'].unique())}\"\n",
    ")\n",
    "print(f\"  Min: {df_clean['augementation_salaire_precedente'].min()}%\")\n",
    "print(f\"  Max: {df_clean['augementation_salaire_precedente'].max()}%\")\n",
    "print(f\"  Moyenne: {df_clean['augementation_salaire_precedente'].mean():.1f}%\")\n",
    "\n",
    "print(\"\\nLa colonne sera maintenant traitee comme NUMERIQUE dans le Pipeline\")\n",
    "print(\"   → StandardScaler au lieu de OneHotEncoder\")\n",
    "print(\"   → 1 feature au lieu de 14\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e791b",
   "metadata": {},
   "source": [
    "## 10. Feature Engineering - Creation de nouvelles variables\n",
    "\n",
    "**Pourquoi creer de nouvelles features ?**\n",
    "\n",
    "\"features supplementaires par rapport aux donnees d'origine\"\n",
    "\n",
    "**Objectif :** Creer des variables qui capturent des **informations metier** que les colonnes brutes ne montrent pas directement.\n",
    "\n",
    "**Features creees (3) :**\n",
    "\n",
    "| Feature                    | Formule                                        | Interpretation metier                            |\n",
    "| -------------------------- | ---------------------------------------------- | ------------------------------------------------ |\n",
    "| `ratio_salaire_experience` | revenu_mensuel / (experience + 1)              | Employe sous-paye par rapport a son experience ? |\n",
    "| `stagnation_poste`         | annees_dans_le_poste - annees_depuis_promotion | Employe bloque sans evolution ?                  |\n",
    "| `satisfaction_globale`     | moyenne des 4 satisfactions employee           | Score synthetique de bien-etre                   |\n",
    "\n",
    "**Pourquoi seulement 3 ?**\n",
    "\n",
    "- Eviter l'**overfitting** (trop de features pour peu de donnees)\n",
    "- Chaque feature doit avoir un **sens metier RH**\n",
    "- Le dataset est deja riche (50+ colonnes apres encodage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9934a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 1 : Ratio salaire / experience\n",
    "# Interpretation : Un ratio bas = potentiellement sous-paye\n",
    "# Note: +1 pour eviter division par zero si experience = 0\n",
    "df_clean[\"ratio_salaire_experience\"] = df_clean[\"revenu_mensuel\"] / (\n",
    "    df_clean[\"annee_experience_totale\"] + 1\n",
    ")\n",
    "\n",
    "# Feature 2 : Stagnation de carriere\n",
    "# Interpretation : Valeur elevee = beaucoup d'annees dans le poste sans promotion recente\n",
    "df_clean[\"stagnation_poste\"] = (\n",
    "    df_clean[\"annees_dans_le_poste_actuel\"]\n",
    "    - df_clean[\"annees_depuis_la_derniere_promotion\"]\n",
    ")\n",
    "\n",
    "# Feature 3 : Satisfaction globale (moyenne des 4 satisfactions)\n",
    "# Interpretation : Score synthetique qui resume le bien-etre au travail\n",
    "cols_satisfaction = [\n",
    "    \"satisfaction_employee_environnement\",\n",
    "    \"satisfaction_employee_nature_travail\",\n",
    "    \"satisfaction_employee_equipe\",\n",
    "    \"satisfaction_employee_equilibre_pro_perso\",\n",
    "]\n",
    "df_clean[\"satisfaction_globale\"] = df_clean[cols_satisfaction].mean(axis=1)\n",
    "\n",
    "print(\"3 features creees avec succes !\")\n",
    "print(f\"\\nNouvelle shape du dataframe : {df_clean.shape}\")\n",
    "print(\"\\nApercu des nouvelles features :\")\n",
    "df_clean[\n",
    "    [\"ratio_salaire_experience\", \"stagnation_poste\", \"satisfaction_globale\"]\n",
    "].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0221235",
   "metadata": {},
   "source": [
    "### 10.2 Verification de la pertinence des features\n",
    "\n",
    "Verifions la correlation de nos nouvelles features avec la variable cible `depart` :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724aa73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation des nouvelles features avec depart (variable cible)\n",
    "new_features = [\"ratio_salaire_experience\", \"stagnation_poste\", \"satisfaction_globale\"]\n",
    "\n",
    "# Creer un dataframe temporaire avec les nouvelles features et la cible\n",
    "temp_df = df_clean[new_features].copy()\n",
    "temp_df[\"depart\"] = df_clean[\n",
    "    \"depart\"\n",
    "].values  # La cible est encore dans df_clean a ce stade\n",
    "\n",
    "correlations = temp_df.corr()[\"depart\"].drop(\"depart\")\n",
    "\n",
    "print(\"Correlation avec depart (variable cible) :\")\n",
    "for feat, corr in correlations.items():\n",
    "    signe = \"🔴\" if corr > 0 else \"🟢\"\n",
    "    interpretation = \"quitte plus\" if corr > 0 else \"reste plus\"\n",
    "    print(f\"{signe} {feat}: {corr:.4f} ({interpretation})\")\n",
    "\n",
    "print(\"\\nInterpretation :\")\n",
    "print(\"   - satisfaction_globale : plus les gens sont satisfaits, moins ils partent\")\n",
    "print(\n",
    "    \"   - stagnation_poste : correlation negative = ceux qui stagnent RESTENT (profils seniors stables)\"\n",
    ")\n",
    "print(\n",
    "    \"   - ratio_salaire_experience : les mieux payes par rapport a leur experience partent plus\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8927c9",
   "metadata": {},
   "source": [
    "## 11. Pipeline de preprocessing avec ColumnTransformer\n",
    "\n",
    "### Pourquoi utiliser un Pipeline ?\n",
    "\n",
    "| Avantage                     | Explication                                                               |\n",
    "| ---------------------------- | ------------------------------------------------------------------------- |\n",
    "| ✅ **Evite le data leakage** | Le preprocessing est applique UNIQUEMENT sur le train a chaque fold de CV |\n",
    "| ✅ **Code propre**           | Tout le preprocessing est encapsule dans un seul objet                    |\n",
    "| ✅ **Reproductible**         | Facile a reutiliser et deployer                                           |\n",
    "| ✅ **Compatible CV**         | S'integre parfaitement avec `cross_val_score` et `GridSearchCV`           |\n",
    "\n",
    "### 11.1 Identification des colonnes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187f5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separation features / cible AVANT le pipeline\n",
    "y = df_clean[\"depart\"]\n",
    "X = df_clean.drop(columns=[\"depart\"])\n",
    "\n",
    "# Identification automatique des colonnes numeriques et categorielles\n",
    "num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "print(\"Colonnes identifiees pour le Pipeline :\")\n",
    "print(\n",
    "    f\"\\n  Numeriques ({len(num_cols)}) : {num_cols[:5]}{'...' if len(num_cols) > 5 else ''}\"\n",
    ")\n",
    "print(f\"\\n  Categorielles ({len(cat_cols)}) :\")\n",
    "for col in cat_cols:\n",
    "    unique_vals = X[col].unique()\n",
    "    print(f\"    - {col} ({len(unique_vals)} modalites)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f44c983",
   "metadata": {},
   "source": [
    "### 11.2 Creation du ColumnTransformer\n",
    "\n",
    "**ColumnTransformer** applique des transformations differentes selon le type de colonne :\n",
    "\n",
    "| Type de colonne  | Transformation                | Parametre                  |\n",
    "| ---------------- | ----------------------------- | -------------------------- |\n",
    "| **Numerique**    | `StandardScaler()`            | Moyenne=0, Ecart-type=1    |\n",
    "| **Categorielle** | `OneHotEncoder(drop='first')` | Evite colinearite parfaite |\n",
    "\n",
    "**Parametre `remainder='passthrough'`** : conserve les colonnes non transformees (si elles existent).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a743e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Creation du ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_cols),  # Standardisation des numeriques\n",
    "        (\n",
    "            \"cat\",\n",
    "            OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\", sparse_output=False),\n",
    "            cat_cols,\n",
    "        ),  # Encodage des categorielles\n",
    "    ],\n",
    "    remainder=\"passthrough\",  # Conserver les autres colonnes si elles existent\n",
    ")\n",
    "\n",
    "print(\"\\nTransformations definies :\")\n",
    "print(f\"  - 'num' : StandardScaler sur {len(num_cols)} colonnes numeriques\")\n",
    "print(f\"  - 'cat' : OneHotEncoder sur {len(cat_cols)} colonnes categorielles\")\n",
    "print(\"\\nLe preprocessor sera FIT sur X_train uniquement (pas de data leakage)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7739e3d5",
   "metadata": {},
   "source": [
    "### 11.3 Split Train/Test avec stratification\n",
    "\n",
    "**❓ Question légitime : Pourquoi faire un split si on fait de la cross-validation ?**\n",
    "\n",
    "On fait les **DEUX** pour des raisons différentes :\n",
    "\n",
    "| Étape                                       | Objectif                                              | Utilisation                          |\n",
    "| ------------------------------------------- | ----------------------------------------------------- | ------------------------------------ |\n",
    "| **Split Train/Test (80/20)**                | Avoir un **jeu de test FINAL jamais touché**          | Évaluation finale du meilleur modèle |\n",
    "| **Cross-validation (sur Train uniquement)** | **Comparer les modèles** et tuner les hyperparamètres | Sélection du meilleur modèle         |\n",
    "\n",
    "**Pourquoi ?** Si on fait la CV sur **tout le dataset**, on n'a plus de données \"fraîches\" pour vérifier si le modèle généralise vraiment. Le test est le **juge final impartial**.\n",
    "\n",
    "**Workflow preprocessing (éviter data leakage) :**\n",
    "\n",
    "| Etape | Action                                   | Explication                                    |\n",
    "| ----- | ---------------------------------------- | ---------------------------------------------- |\n",
    "| 1     | `X, y = separation features/cible`       | Séparer les variables explicatives de la cible |\n",
    "| 2     | `X_train, X_test = split(X, y)`          | Split **avant** preprocessing                  |\n",
    "| 3     | `preprocessor.fit(X_train)`              | Fit sur train **UNIQUEMENT**                   |\n",
    "| 4     | `X_train_processed = transform(X_train)` | Transform train                                |\n",
    "| 5     | `X_test_processed = transform(X_test)`   | Transform test (mêmes paramètres du train)     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5695bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split stratifie AVANT le fit du preprocessor\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y,  # Important pour le desequilibre de classes\n",
    ")\n",
    "\n",
    "print(\"Split train/test avec stratification :\")\n",
    "print(f\"\\n  Train : {X_train.shape[0]} lignes ({X_train.shape[0] / len(X) * 100:.0f}%)\")\n",
    "print(f\"    - Churn : {y_train.sum()} ({y_train.mean() * 100:.1f}%)\")\n",
    "print(\n",
    "    f\"    - Non-churn : {len(y_train) - y_train.sum()} ({(1 - y_train.mean()) * 100:.1f}%)\"\n",
    ")\n",
    "\n",
    "print(f\"\\n  Test : {X_test.shape[0]} lignes ({X_test.shape[0] / len(X) * 100:.0f}%)\")\n",
    "print(f\"    - Churn : {y_test.sum()} ({y_test.mean() * 100:.1f}%)\")\n",
    "print(\n",
    "    f\"    - Non-churn : {len(y_test) - y_test.sum()} ({(1 - y_test.mean()) * 100:.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7f810e",
   "metadata": {},
   "source": [
    "### 11.4 Application du preprocessor (fit_transform sur train, transform sur test)\n",
    "\n",
    "**C'est ici que la magie du Pipeline opere :**\n",
    "\n",
    "- `fit_transform(X_train)` : calcule les parametres (moyenne, ecart-type, modalites) ET transforme\n",
    "- `transform(X_test)` : utilise les parametres du train pour transformer le test\n",
    "\n",
    "**Avantage majeur** : Quand on utilisera `cross_val_score`, le fit sera automatiquement refait sur chaque fold !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1353beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit sur train, transform sur train et test\n",
    "X_train_processed = preprocessor.fit_transform(X_train)  # FIT + TRANSFORM\n",
    "X_test_processed = preprocessor.transform(X_test)  # TRANSFORM seulement\n",
    "\n",
    "# Recuperation des noms de colonnes pour lisibilite\n",
    "# (Les colonnes numeriques gardent leur nom, les categorielles sont encodees)\n",
    "num_feature_names = num_cols\n",
    "cat_feature_names = (\n",
    "    preprocessor.named_transformers_[\"cat\"].get_feature_names_out(cat_cols).tolist()\n",
    ")\n",
    "all_feature_names = num_feature_names + cat_feature_names\n",
    "\n",
    "print(\"Preprocessing applique avec succes !\")\n",
    "print(\"\\nDimensions apres transformation :\")\n",
    "print(f\"  X_train_processed : {X_train_processed.shape}\")\n",
    "print(f\"  X_test_processed  : {X_test_processed.shape}\")\n",
    "\n",
    "print(f\"\\nNombre de features finales : {len(all_feature_names)}\")\n",
    "print(f\"  - Numeriques (standardisees) : {len(num_feature_names)}\")\n",
    "print(f\"  - Categorielles (encodees)   : {len(cat_feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b653394",
   "metadata": {},
   "source": [
    "### 11.5 Verification : pas de data leakage\n",
    "\n",
    "Verifions que le StandardScaler a bien ete fit sur le train uniquement :\n",
    "\n",
    "- **Train** : moyenne ≈ 0, ecart-type ≈ 1\n",
    "- **Test** : moyenne ≠ 0 exactement (normal, car les parametres viennent du train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e07bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_indices = list(range(len(num_cols)))\n",
    "\n",
    "print(\"Verification du StandardScaler (colonnes numeriques) :\")\n",
    "print(f\"\\n  Train - Moyenne  : {X_train_processed[:, num_indices].mean():.6f}\")\n",
    "print(f\"  Train - Std      : {X_train_processed[:, num_indices].std():.6f}\")\n",
    "print(f\"\\n  Test  - Moyenne  : {X_test_processed[:, num_indices].mean():.6f}\")\n",
    "print(f\"  Test  - Std      : {X_test_processed[:, num_indices].std():.6f}\")\n",
    "\n",
    "print(\"\\nPas de data leakage : le test n'a pas exactement moyenne=0\")\n",
    "print(\"   (les parametres du scaler viennent du train)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904c643c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Synthèse Partie 2 : Feature Engineering\n",
    "\n",
    "### Nettoyage effectué\n",
    "\n",
    "- ✅ Aucun doublon détecté\n",
    "- ✅ Outliers conservés (valeurs RH légitimes : hauts salaires, ancienneté élevée)\n",
    "- ✅ 8 colonnes supprimées : identifiants, variance nulle, redondantes avec la cible\n",
    "\n",
    "### Analyse des corrélations\n",
    "\n",
    "- Matrice de **Pearson** (corrélations linéaires) - visualisation Plotly\n",
    "- Matrice de **Spearman** (corrélations monotones)\n",
    "- Corrélations fortes détectées (|r| > 0.7) mais **variables conservées**\n",
    "- **Justification** : les modèles à base d'arbres gèrent bien la multicolinéarité, et même 5% d'information unique peut être utile\n",
    "\n",
    "### Conversion de type (best practice)\n",
    "\n",
    "- ✅ `augementation_salaire_precedente` : \"11 %\" → 11.0 (numérique)\n",
    "- **Avantage** : 1 feature standardisée au lieu de 14 colonnes OneHot → moins d'overfitting\n",
    "- Le modèle comprend que 23% > 11% (ordre préservé)\n",
    "\n",
    "### Feature Engineering (3 nouvelles features métier)\n",
    "\n",
    "| Feature                    | Formule                 | Corr. avec depart | Interprétation                      |\n",
    "| -------------------------- | ----------------------- | ----------------- | ----------------------------------- |\n",
    "| `ratio_salaire_experience` | salaire / (exp + 1)     | +0.10             | Bien payés partent plus             |\n",
    "| `stagnation_poste`         | ancienneté - promotion  | -0.15             | Stagnants restent (profils stables) |\n",
    "| `satisfaction_globale`     | moyenne 4 satisfactions | -0.16             | Satisfaits restent                  |\n",
    "\n",
    "### Pipeline de preprocessing (ColumnTransformer)\n",
    "\n",
    "| Type       | Transformation                | Colonnes             |\n",
    "| ---------- | ----------------------------- | -------------------- |\n",
    "| Numérique  | `StandardScaler()`            | 23 colonnes          |\n",
    "| Catégoriel | `OneHotEncoder(drop='first')` | 7 cols → 21 features |\n",
    "\n",
    "### Dataset prêt pour la modélisation\n",
    "\n",
    "| Métrique         | Valeur                             |\n",
    "| ---------------- | ---------------------------------- |\n",
    "| Features totales | **44** (23 num + 21 cat)           |\n",
    "| Train            | 1176 lignes (80%)                  |\n",
    "| Test             | 294 lignes (20%)                   |\n",
    "| Churn train      | 16.2%                              |\n",
    "| Churn test       | 16.0%                              |\n",
    "| Data leakage     | Vérifié (fit sur train uniquement) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14188a43",
   "metadata": {},
   "source": [
    "# Partie 3 : Modélisation de Référence (Baseline)\n",
    "\n",
    "**Objectif :** Établir des **modèles de référence** pour comprendre la difficulté du problème avant d'optimiser.\n",
    "\n",
    "---\n",
    "\n",
    "## Approche méthodologique\n",
    "\n",
    "| Étape | Modèle                     | Objectif                                                  |\n",
    "| ----- | -------------------------- | --------------------------------------------------------- |\n",
    "| 1     | **DummyClassifier**        | Baseline naïf (que vaut \"toujours prédire majoritaire\" ?) |\n",
    "| 2     | **LogisticRegression**     | Modèle linéaire simple                                    |\n",
    "| 3     | **RandomForestClassifier** | Modèle non-linéaire (arbres)                              |\n",
    "\n",
    "## Métriques utilisées\n",
    "\n",
    "Pour un problème de **classification binaire déséquilibrée** (16% churn), l'**accuracy** est trompeuse.\n",
    "\n",
    "| Métrique      | Formule               | Interprétation métier                      |\n",
    "| ------------- | --------------------- | ------------------------------------------ |\n",
    "| **Precision** | TP / (TP + FP)        | \"Parmi les alertes, combien sont vraies ?\" |\n",
    "| **Recall**    | TP / (TP + FN)        | \"Combien de départs réels détectés ?\"      |\n",
    "| **F1-Score**  | 2 × (P × R) / (P + R) | Équilibre Precision/Recall                 |\n",
    "\n",
    "**Contexte RH :** Le **Recall** est critique car **rater un départ** (FN) coûte plus cher qu'une fausse alerte (FP).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de17a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import (\n",
    "    make_scorer,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "def evaluate_model_cv(\n",
    "    model,\n",
    "    X_data,\n",
    "    y_data,\n",
    "    preprocessor,\n",
    "    resampler=None,\n",
    "    cv=5,\n",
    "    model_name=\"Model\",\n",
    "    pos_label=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Évalue un modèle avec validation croisée stratifiée.\n",
    "    Si resampler est fourni (SMOTE, RandomUnderSampler...), l'applique sur le train.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : estimator sklearn\n",
    "    X_data : DataFrame des features\n",
    "    y_data : Series de la cible\n",
    "    preprocessor : ColumnTransformer pour le preprocessing\n",
    "    resampler : imblearn resampler (SMOTE, RandomUnderSampler...) ou None\n",
    "    cv : int, nombre de folds\n",
    "    model_name : str, nom du modèle pour l'affichage\n",
    "    pos_label : int, label de la classe positive (défaut: 1)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : résultats avec moyennes et écarts-types (test uniquement si resampling)\n",
    "    \"\"\"\n",
    "    from sklearn.pipeline import Pipeline\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "    # Si pas de resampling, utiliser cross_validate classique (plus rapide)\n",
    "    if resampler is None:\n",
    "        pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", model)])\n",
    "\n",
    "        scoring = {\n",
    "            \"recall\": make_scorer(recall_score, pos_label=pos_label),\n",
    "            \"precision\": make_scorer(\n",
    "                precision_score, pos_label=pos_label, zero_division=0\n",
    "            ),\n",
    "            \"f1\": make_scorer(f1_score, pos_label=pos_label),\n",
    "            \"roc_auc\": \"roc_auc\",\n",
    "        }\n",
    "\n",
    "        cv_results = cross_validate(\n",
    "            pipeline,\n",
    "            X_data,\n",
    "            y_data,\n",
    "            cv=skf,\n",
    "            scoring=scoring,\n",
    "            return_train_score=True,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "\n",
    "        results = {\n",
    "            \"model\": model_name,\n",
    "            \"recall_train_mean\": cv_results[\"train_recall\"].mean(),\n",
    "            \"recall_train_std\": cv_results[\"train_recall\"].std(),\n",
    "            \"recall_test_mean\": cv_results[\"test_recall\"].mean(),\n",
    "            \"recall_test_std\": cv_results[\"test_recall\"].std(),\n",
    "            \"precision_train_mean\": cv_results[\"train_precision\"].mean(),\n",
    "            \"precision_train_std\": cv_results[\"train_precision\"].std(),\n",
    "            \"precision_test_mean\": cv_results[\"test_precision\"].mean(),\n",
    "            \"precision_test_std\": cv_results[\"test_precision\"].std(),\n",
    "            \"f1_train_mean\": cv_results[\"train_f1\"].mean(),\n",
    "            \"f1_train_std\": cv_results[\"train_f1\"].std(),\n",
    "            \"f1_test_mean\": cv_results[\"test_f1\"].mean(),\n",
    "            \"f1_test_std\": cv_results[\"test_f1\"].std(),\n",
    "            \"roc_auc_train_mean\": cv_results[\"train_roc_auc\"].mean(),\n",
    "            \"roc_auc_test_mean\": cv_results[\"test_roc_auc\"].mean(),\n",
    "        }\n",
    "\n",
    "        # Affichage avec train et test\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\" {model_name}\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "        print(f\"\\n{'Métrique':<15} {'Train':>20} {'Test':>20}\")\n",
    "        print(\"-\" * 55)\n",
    "        print(\n",
    "            f\"{'Recall':<15} {results['recall_train_mean']:.3f} ± {results['recall_train_std']:.3f}    {results['recall_test_mean']:.3f} ± {results['recall_test_std']:.3f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"{'Precision':<15} {results['precision_train_mean']:.3f} ± {results['precision_train_std']:.3f}    {results['precision_test_mean']:.3f} ± {results['precision_test_std']:.3f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"{'F1-Score':<15} {results['f1_train_mean']:.3f} ± {results['f1_train_std']:.3f}    {results['f1_test_mean']:.3f} ± {results['f1_test_std']:.3f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"{'ROC-AUC':<15} {results['roc_auc_train_mean']:.3f}              {results['roc_auc_test_mean']:.3f}\"\n",
    "        )\n",
    "    else:\n",
    "        # Avec resampling : boucle manuelle, métriques TEST uniquement\n",
    "        # (prédire sur train n'a pas de sens car le modèle a déjà vu ces données)\n",
    "        y_array = y_data.values if hasattr(y_data, \"values\") else y_data\n",
    "\n",
    "        metrics = {\n",
    "            \"recall_test\": [],\n",
    "            \"precision_test\": [],\n",
    "            \"f1_test\": [],\n",
    "            \"roc_auc_test\": [],\n",
    "        }\n",
    "\n",
    "        for train_idx, test_idx in skf.split(X_data, y_array):\n",
    "            X_train_fold, X_test_fold = X_data.iloc[train_idx], X_data.iloc[test_idx]\n",
    "            y_train_fold, y_test_fold = y_array[train_idx], y_array[test_idx]\n",
    "\n",
    "            # Preprocessing\n",
    "            X_train_processed = preprocessor.fit_transform(X_train_fold)\n",
    "            X_test_processed = preprocessor.transform(X_test_fold)\n",
    "\n",
    "            # Resampling sur train uniquement\n",
    "            X_train_resampled, y_train_resampled = resampler.fit_resample(\n",
    "                X_train_processed, y_train_fold\n",
    "            )\n",
    "\n",
    "            # Entraînement\n",
    "            clf = clone(model)\n",
    "            clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "            # Prédictions sur TEST uniquement (jamais sur train !)\n",
    "            y_test_pred = clf.predict(X_test_processed)\n",
    "\n",
    "            # Métriques sur TEST\n",
    "            metrics[\"recall_test\"].append(\n",
    "                recall_score(y_test_fold, y_test_pred, pos_label=pos_label)\n",
    "            )\n",
    "            metrics[\"precision_test\"].append(\n",
    "                precision_score(\n",
    "                    y_test_fold, y_test_pred, pos_label=pos_label, zero_division=0\n",
    "                )\n",
    "            )\n",
    "            metrics[\"f1_test\"].append(\n",
    "                f1_score(y_test_fold, y_test_pred, pos_label=pos_label)\n",
    "            )\n",
    "\n",
    "            if hasattr(clf, \"predict_proba\"):\n",
    "                pos_idx = list(clf.classes_).index(pos_label)\n",
    "                y_test_proba = clf.predict_proba(X_test_processed)[:, pos_idx]\n",
    "                metrics[\"roc_auc_test\"].append(\n",
    "                    roc_auc_score((y_test_fold == pos_label).astype(int), y_test_proba)\n",
    "                )\n",
    "\n",
    "        results = {\n",
    "            \"model\": model_name,\n",
    "            \"recall_test_mean\": np.mean(metrics[\"recall_test\"]),\n",
    "            \"recall_test_std\": np.std(metrics[\"recall_test\"]),\n",
    "            \"precision_test_mean\": np.mean(metrics[\"precision_test\"]),\n",
    "            \"precision_test_std\": np.std(metrics[\"precision_test\"]),\n",
    "            \"f1_test_mean\": np.mean(metrics[\"f1_test\"]),\n",
    "            \"f1_test_std\": np.std(metrics[\"f1_test\"]),\n",
    "            \"roc_auc_test_mean\": np.mean(metrics[\"roc_auc_test\"])\n",
    "            if metrics[\"roc_auc_test\"]\n",
    "            else 0,\n",
    "        }\n",
    "\n",
    "        # Affichage TEST uniquement (pas de train avec resampling)\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\" {model_name}\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "        print(f\"\\n{'Métrique':<15} {'Test (CV)':>20}\")\n",
    "        print(\"-\" * 35)\n",
    "        print(\n",
    "            f\"{'Recall':<15} {results['recall_test_mean']:.3f} ± {results['recall_test_std']:.3f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"{'Precision':<15} {results['precision_test_mean']:.3f} ± {results['precision_test_std']:.3f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"{'F1-Score':<15} {results['f1_test_mean']:.3f} ± {results['f1_test_std']:.3f}\"\n",
    "        )\n",
    "        print(f\"{'ROC-AUC':<15} {results['roc_auc_test_mean']:.3f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Fonction evaluate_model_cv() définie (avec support optionnel du resampling)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a86fda6",
   "metadata": {},
   "source": [
    "## 12. Modèle Dummy (Baseline naïf)\n",
    "\n",
    "Le `DummyClassifier` sert de **référence minimale**. Si nos vrais modèles ne font pas mieux, c'est qu'ils n'apprennent rien.\n",
    "\n",
    "**Stratégie utilisée :** `most_frequent` → prédit toujours la classe majoritaire (0 = resté)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb7de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# DummyClassifier avec validation croisée (comme référence)\n",
    "print(\"🔄 Évaluation DummyClassifier avec validation croisée...\")\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"stratified\", random_state=42)\n",
    "\n",
    "# Utilisation de la fonction evaluate_model_cv définie plus haut\n",
    "results_dummy = evaluate_model_cv(\n",
    "    dummy_clf, X, y, preprocessor, model_name=\"DummyClassifier (stratified)\"\n",
    ")\n",
    "\n",
    "print(\"\\n⚠️ Ce modèle sert de BASELINE - tout modèle doit faire mieux !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aed7e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Logistic Regression sans class_weight (baseline)\n",
    "print(\n",
    "    \"🔄 Évaluation Logistic Regression (sans class_weight) avec validation croisée...\"\n",
    ")\n",
    "\n",
    "lr_clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "results_lr = evaluate_model_cv(\n",
    "    lr_clf, X, y, preprocessor, model_name=\"Logistic Regression (sans class_weight)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbff1db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest sans class_weight (baseline)\n",
    "print(\"🔄 Évaluation Random Forest (sans class_weight) avec validation croisée...\")\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "results_rf = evaluate_model_cv(\n",
    "    rf_clf, X, y, preprocessor, model_name=\"Random Forest (sans class_weight)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81795d7c",
   "metadata": {},
   "source": [
    "## 13. Comparaison des Modèles Baseline\n",
    "\n",
    "Récapitulatif des performances des 3 modèles **sans gestion du déséquilibre**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc86995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des modèles baseline (résultats de cross-validation)\n",
    "baseline_results = [results_dummy, results_lr, results_rf]\n",
    "\n",
    "comparison_df = pd.DataFrame(baseline_results)\n",
    "\n",
    "# Sélection des colonnes principales\n",
    "display_cols = [\n",
    "    \"model\",\n",
    "    \"recall_test_mean\",\n",
    "    \"recall_test_std\",\n",
    "    \"precision_test_mean\",\n",
    "    \"f1_test_mean\",\n",
    "    \"roc_auc_test_mean\",\n",
    "]\n",
    "\n",
    "comparison_display = comparison_df[display_cols].copy()\n",
    "comparison_display.columns = [\n",
    "    \"Modèle\",\n",
    "    \"Recall (CV)\",\n",
    "    \"Recall Std\",\n",
    "    \"Precision (CV)\",\n",
    "    \"F1 (CV)\",\n",
    "    \"ROC-AUC (CV)\",\n",
    "]\n",
    "\n",
    "# Formatage\n",
    "comparison_display[\"Recall (CV)\"] = comparison_display.apply(\n",
    "    lambda x: f\"{x['Recall (CV)']:.3f} ± {x['Recall Std']:.3f}\", axis=1\n",
    ")\n",
    "comparison_display = comparison_display.drop(\"Recall Std\", axis=1)\n",
    "\n",
    "print(\"COMPARAISON DES MODÈLES BASELINE (sans gestion du déséquilibre)\")\n",
    "print(\"\\nMétriques en validation croisée (5 folds) - Focus classe 'Parti'\\n\")\n",
    "\n",
    "display(comparison_display)\n",
    "\n",
    "print(\"CONSTAT IMPORTANT :\")\n",
    "print(\"\\n• L'Accuracy est élevée (~84%) mais TROMPEUSE !\")\n",
    "print(\"  → Un modèle qui prédit TOUJOURS 'Resté' aurait aussi ~84% d'accuracy\")\n",
    "print(\"\\n• Le Recall est CATASTROPHIQUE (proche de 0)\")\n",
    "print(\"  → Les modèles ratent presque TOUS les départs\")\n",
    "print(\"\\n• Conclusion : Il FAUT gérer le déséquilibre des classes !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73556c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation comparative des modèles baseline\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Préparation des données à partir des résultats de cross-validation\n",
    "models = [\"Dummy\", \"Logistic Reg.\", \"Random Forest\"]\n",
    "recall_test = [\n",
    "    results_dummy[\"recall_test_mean\"],\n",
    "    results_lr[\"recall_test_mean\"],\n",
    "    results_rf[\"recall_test_mean\"],\n",
    "]\n",
    "precision_test = [\n",
    "    results_dummy[\"precision_test_mean\"],\n",
    "    results_lr[\"precision_test_mean\"],\n",
    "    results_rf[\"precision_test_mean\"],\n",
    "]\n",
    "f1_test = [\n",
    "    results_dummy[\"f1_test_mean\"],\n",
    "    results_lr[\"f1_test_mean\"],\n",
    "    results_rf[\"f1_test_mean\"],\n",
    "]\n",
    "recall_train = [\n",
    "    results_dummy[\"recall_train_mean\"],\n",
    "    results_lr[\"recall_train_mean\"],\n",
    "    results_rf[\"recall_train_mean\"],\n",
    "]\n",
    "\n",
    "# Graphique 1 : Métriques sur la classe 'Parti' (Test)\n",
    "x = np.arange(len(models))\n",
    "width = 0.25\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.bar(x - width, precision_test, width, label=\"Precision\", color=\"steelblue\")\n",
    "ax1.bar(x, recall_test, width, label=\"Recall\", color=\"darkorange\")\n",
    "ax1.bar(x + width, f1_test, width, label=\"F1-Score\", color=\"green\")\n",
    "ax1.set_ylabel(\"Score\")\n",
    "ax1.set_title(\"Métriques sur la classe 'Parti' (Test CV)\")\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models)\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.axhline(y=0.5, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Seuil 50%\")\n",
    "\n",
    "# Graphique 2 : Comparaison Train vs Test (détection overfitting)\n",
    "ax2 = axes[1]\n",
    "x2 = np.arange(len(models))\n",
    "ax2.bar(x2 - 0.2, recall_train, 0.4, label=\"Train\", color=\"steelblue\")\n",
    "ax2.bar(x2 + 0.2, recall_test, 0.4, label=\"Test\", color=\"darkorange\")\n",
    "ax2.set_ylabel(\"Recall (Parti)\")\n",
    "ax2.set_title(\"Détection Overfitting : Train vs Test\")\n",
    "ax2.set_xticks(x2)\n",
    "ax2.set_xticklabels(models)\n",
    "ax2.legend()\n",
    "ax2.set_ylim(0, 1.1)\n",
    "\n",
    "# Annotations pour Random Forest (overfitting)\n",
    "ax2.annotate(\n",
    "    \"Overfitting!\",\n",
    "    xy=(2, recall_train[2]),\n",
    "    xytext=(2, recall_train[2] + 0.05),\n",
    "    ha=\"center\",\n",
    "    fontsize=10,\n",
    "    color=\"red\",\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d50886",
   "metadata": {},
   "source": [
    "### 13.1 Analyse des résultats Baseline\n",
    "\n",
    "**Constats observés :**\n",
    "\n",
    "| Modèle                  | Comportement observé                              |\n",
    "| ----------------------- | ------------------------------------------------- |\n",
    "| **Dummy**               | Recall ~14% (prédiction aléatoire stratifiée)     |\n",
    "| **Logistic Regression** | Recall ~40%, meilleur F1 parmi les baselines      |\n",
    "| **Random Forest**       | Recall train=100% vs test=16% (**overfitting !**) |\n",
    "\n",
    "**Problème identifié :** Sans gestion du déséquilibre (16% vs 84%), les modèles tendent à ignorer la classe minoritaire \"Parti\" ou à surapprendre sur le train set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcb19c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Synthèse Partie 3 : Modélisation Baseline\n",
    "\n",
    "### Modèles entraînés\n",
    "\n",
    "| Modèle                   | Type                  | Objectif                     |\n",
    "| ------------------------ | --------------------- | ---------------------------- |\n",
    "| `DummyClassifier`        | Baseline naïf         | Référence minimale           |\n",
    "| `LogisticRegression`     | Linéaire              | Premier modèle interprétable |\n",
    "| `RandomForestClassifier` | Non-linéaire (arbres) | Capture relations complexes  |\n",
    "\n",
    "### Métriques calculées\n",
    "\n",
    "- ✅ `classification_report()` (Precision, Recall, F1)\n",
    "- ✅ Matrice de confusion (Train ET Test)\n",
    "- ✅ Comparaison Train vs Test pour détecter l'overfitting\n",
    "\n",
    "### Problème identifié\n",
    "\n",
    "**Déséquilibre des classes (16% Parti / 84% Resté)** :\n",
    "\n",
    "- Les modèles optimisent l'accuracy → ignorent la classe minoritaire\n",
    "- Le Recall sur \"Parti\" est insuffisant pour un usage métier RH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b2c64",
   "metadata": {},
   "source": [
    "# Partie 4 : Gestion du Déséquilibre des Classes\n",
    "\n",
    "---\n",
    "\n",
    "## 14. Stratégie de gestion du déséquilibre\n",
    "\n",
    "### 14.1 Contexte métier\n",
    "\n",
    "Dans un contexte RH, **identifier les employés à risque de départ** (classe \"Parti\") est crucial :\n",
    "\n",
    "- Un **Faux Négatif** (employé à risque non détecté) = départ non anticipé → coût élevé\n",
    "- Un **Faux Positif** (employé stable mal classé) = actions RH inutiles → coût modéré\n",
    "\n",
    "**Objectif** : Maximiser le **Recall** sur la classe \"Parti\" tout en maintenant une Precision acceptable.\n",
    "\n",
    "### 14.2 Techniques à tester\n",
    "\n",
    "| #   | Technique                 | Package  | Approche                     |\n",
    "| --- | ------------------------- | -------- | ---------------------------- |\n",
    "| 1   | `class_weight='balanced'` | sklearn  | Pondération des erreurs      |\n",
    "| 2   | SMOTE                     | imblearn | Oversampling synthétique     |\n",
    "| 3   | Random Undersampling      | imblearn | Réduction classe majoritaire |\n",
    "\n",
    "### 14.3 Protocole d'évaluation\n",
    "\n",
    "- **Validation croisée stratifiée** : `StratifiedKFold` (5 folds)\n",
    "- **Métriques principales** : Recall, Precision, F1-score (classe \"Parti\")\n",
    "- **Métrique secondaire** : ROC-AUC, PR-AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068a8387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline as SkPipeline\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "print(\"Distribution des classes :\")\n",
    "print(y.value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3828bb",
   "metadata": {},
   "source": [
    "## 15. Technique 1 : Class Weight (Pondération des classes)\n",
    "\n",
    "### 15.1 Principe\n",
    "\n",
    "Le paramètre `class_weight='balanced'` ajuste automatiquement les poids des classes inversement proportionnels à leur fréquence :\n",
    "\n",
    "$$w_c = \\frac{n_{samples}}{n_{classes} \\times n_{samples_c}}$$\n",
    "\n",
    "Cela pénalise davantage les erreurs sur la classe minoritaire (\"Parti\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c47e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18.2 Entraînement des modèles avec class_weight='balanced'\n",
    "all_results = []  # Liste pour stocker tous les résultats\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"🔄 Entraînement Logistic Regression avec class_weight='balanced'...\")\n",
    "lr_balanced = LogisticRegression(\n",
    "    class_weight=\"balanced\", max_iter=1000, random_state=42\n",
    ")\n",
    "results_lr_balanced = evaluate_model_cv(\n",
    "    lr_balanced, X, y, preprocessor, model_name=\"Logistic Regression (balanced)\"\n",
    ")\n",
    "all_results.append(results_lr_balanced)\n",
    "\n",
    "# Random Forest\n",
    "print(\"\\n🔄 Entraînement Random Forest avec class_weight='balanced'...\")\n",
    "rf_balanced = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42, n_jobs=-1\n",
    ")\n",
    "results_rf_balanced = evaluate_model_cv(\n",
    "    rf_balanced, X, y, preprocessor, model_name=\"Random Forest (balanced)\"\n",
    ")\n",
    "all_results.append(results_rf_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4491e96b",
   "metadata": {},
   "source": [
    "## 16. Technique 2 : SMOTE (Oversampling)\n",
    "\n",
    "### 16.1 Principe\n",
    "\n",
    "**SMOTE** (Synthetic Minority Over-sampling Technique) génère des observations synthétiques pour la classe minoritaire en interpolant entre les observations existantes et leurs k plus proches voisins.\n",
    "\n",
    "**Important** : SMOTE doit être appliqué **uniquement sur le jeu d'entraînement** pour éviter la fuite de données (data leakage).\n",
    "\n",
    "**Note** : On utilise la même fonction `evaluate_model_cv()` avec le paramètre optionnel `resampler` pour éviter la duplication de code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b4bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17.2 Random Forest avec SMOTE\n",
    "print(\"🔄 Entraînement Random Forest avec SMOTE...\")\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "rf_smote = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# On utilise evaluate_model_cv avec le paramètre resampler\n",
    "results_rf_smote = evaluate_model_cv(\n",
    "    rf_smote, X, y, preprocessor, resampler=smote, model_name=\"Random Forest + SMOTE\"\n",
    ")\n",
    "all_results.append(results_rf_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ee94a",
   "metadata": {},
   "source": [
    "## 17. Technique 3 : Undersampling\n",
    "\n",
    "### 17.1 Principe\n",
    "\n",
    "L'**undersampling** réduit le nombre d'observations de la classe majoritaire (\"Resté\") pour équilibrer les classes.\n",
    "\n",
    "⚠️ **Inconvénient** : Perte d'information en supprimant des données.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0760963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18.2 Random Forest avec Undersampling\n",
    "print(\"Entraînement Random Forest avec Undersampling...\")\n",
    "\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "\n",
    "rf_under = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# On utilise evaluate_model_cv avec le paramètre resampler\n",
    "results_rf_under = evaluate_model_cv(\n",
    "    rf_under,\n",
    "    X,\n",
    "    y,\n",
    "    preprocessor,\n",
    "    resampler=undersampler,\n",
    "    model_name=\"Random Forest + Undersampling\",\n",
    ")\n",
    "all_results.append(results_rf_under)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b773b542",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 18. Comparaison des techniques de gestion du déséquilibre\n",
    "\n",
    "### 18.1 Tableau récapitulatif\n",
    "\n",
    "### 18.2 Comment lire les graphiques\n",
    "\n",
    "**Graphique 1 - Trade-off Recall vs Precision :**\n",
    "\n",
    "- **Axe X** : Recall (capacité à détecter les départs)\n",
    "- **Axe Y** : Precision (fiabilité des alertes)\n",
    "- **Lecture** : Un point en haut à droite = modèle idéal (bon recall ET bonne precision)\n",
    "- **Objectif métier** : Privilégier le recall (détecter un maximum de départs)\n",
    "\n",
    "**Graphique 2 - F1-Score par technique :**\n",
    "\n",
    "- **Barres horizontales** : Score F1 (moyenne harmonique recall/precision)\n",
    "- **Barres d'erreur** : Écart-type sur les 5 folds de validation croisée\n",
    "- **Lecture** : Plus la barre est longue, meilleur est le compromis recall/precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e74622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du DataFrame de comparaison\n",
    "comparison_imbalance_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Sélection et formatage des colonnes principales\n",
    "display_cols = [\n",
    "    \"model\",\n",
    "    \"recall_test_mean\",\n",
    "    \"recall_test_std\",\n",
    "    \"precision_test_mean\",\n",
    "    \"f1_test_mean\",\n",
    "    \"roc_auc_test_mean\",\n",
    "]\n",
    "\n",
    "comparison_display = comparison_imbalance_df[display_cols].copy()\n",
    "comparison_display.columns = [\n",
    "    \"Modèle\",\n",
    "    \"Recall (Test)\",\n",
    "    \"Recall Std\",\n",
    "    \"Precision (Test)\",\n",
    "    \"F1 (Test)\",\n",
    "    \"ROC-AUC (Test)\",\n",
    "]\n",
    "\n",
    "# Formatage pour affichage\n",
    "comparison_display[\"Recall (Test)\"] = comparison_display.apply(\n",
    "    lambda x: f\"{x['Recall (Test)']:.3f} ± {x['Recall Std']:.3f}\", axis=1\n",
    ")\n",
    "comparison_display = comparison_display.drop(\"Recall Std\", axis=1)\n",
    "\n",
    "print(\"COMPARAISON DES TECHNIQUES DE GESTION DU DÉSÉQUILIBRE\")\n",
    "print(\"\\nFocus sur la classe 'Parti' (classe minoritaire)\\n\")\n",
    "\n",
    "display(\n",
    "    comparison_display.style.highlight_max(\n",
    "        subset=[\"F1 (Test)\", \"ROC-AUC (Test)\"], color=\"green\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1b537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22.2 Visualisation graphique\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Graphique 1 : Recall vs Precision (Trade-off)\n",
    "ax1 = axes[0]\n",
    "models = comparison_imbalance_df[\"model\"].values\n",
    "recalls = comparison_imbalance_df[\"recall_test_mean\"].values\n",
    "precisions = comparison_imbalance_df[\"precision_test_mean\"].values\n",
    "\n",
    "colors_plot = plt.cm.Set2(np.linspace(0, 1, len(models)))\n",
    "for i, (model, recall, precision) in enumerate(zip(models, recalls, precisions)):\n",
    "    ax1.scatter(\n",
    "        recall,\n",
    "        precision,\n",
    "        s=200,\n",
    "        c=[colors_plot[i]],\n",
    "        label=model,\n",
    "        edgecolors=\"black\",\n",
    "        linewidth=1.5,\n",
    "    )\n",
    "\n",
    "ax1.set_xlabel(\"Recall (Test)\", fontsize=12)\n",
    "ax1.set_ylabel(\"Precision (Test)\", fontsize=12)\n",
    "ax1.set_title(\"Trade-off Recall vs Precision\", fontsize=14, fontweight=\"bold\")\n",
    "ax1.legend(loc=\"best\", fontsize=8)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Graphique 2 : F1-Score par modèle\n",
    "ax2 = axes[1]\n",
    "f1_scores = comparison_imbalance_df[\"f1_test_mean\"].values\n",
    "f1_stds = comparison_imbalance_df[\"f1_test_std\"].values\n",
    "\n",
    "bars = ax2.barh(\n",
    "    range(len(models)), f1_scores, xerr=f1_stds, color=colors_plot, edgecolor=\"black\"\n",
    ")\n",
    "ax2.set_yticks(range(len(models)))\n",
    "ax2.set_yticklabels(\n",
    "    [m.replace(\" + \", \"\\n+ \").replace(\" (\", \"\\n(\") for m in models], fontsize=9\n",
    ")\n",
    "ax2.set_xlabel(\"F1-Score (Test)\", fontsize=12)\n",
    "ax2.set_title(\"F1-Score par technique\", fontsize=14, fontweight=\"bold\")\n",
    "ax2.grid(True, alpha=0.3, axis=\"x\")\n",
    "\n",
    "# Ajout des valeurs sur les barres\n",
    "for i, (bar, f1) in enumerate(zip(bars, f1_scores)):\n",
    "    ax2.text(\n",
    "        f1 + 0.02,\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f\"{f1:.3f}\",\n",
    "        va=\"center\",\n",
    "        fontsize=10,\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualisation des performances terminée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f56f12",
   "metadata": {},
   "source": [
    "### 18.3 Analyse des résultats\n",
    "\n",
    "| Technique                   | Avantages                                   | Inconvénients                     |\n",
    "| --------------------------- | ------------------------------------------- | --------------------------------- |\n",
    "| **class_weight='balanced'** | Simple, pas de preprocessing supplémentaire | Peut ne pas suffire seul          |\n",
    "| **SMOTE**                   | Génère des données synthétiques             | Risque d'overfitting, coût calcul |\n",
    "| **Undersampling**           | Réduit le temps d'entraînement              | Perte d'information               |\n",
    "\n",
    "#### Observations :\n",
    "\n",
    "1. **Recall** : Les techniques de gestion du déséquilibre améliorent significativement le recall sur \"Parti\"\n",
    "2. **Trade-off** : Amélioration du recall souvent au détriment de la precision\n",
    "3. **F1-Score** : Compromis entre recall et precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c74354",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 19. Courbe Precision-Recall\n",
    "\n",
    "### 19.1 Principe\n",
    "\n",
    "La courbe Precision-Recall permet de visualiser le trade-off entre la détection des départs (Recall) et la fiabilité des alertes (Precision).\n",
    "\n",
    "### 19.2 Comment lire les graphiques\n",
    "\n",
    "**Graphique 1 - Courbe Precision-Recall :**\n",
    "\n",
    "- **Axe X** : Recall (proportion des vrais départs détectés)\n",
    "- **Axe Y** : Precision (proportion des alertes qui sont de vrais départs)\n",
    "- **Aire sous la courbe (AP)** : Plus elle est grande, meilleur est le modèle\n",
    "- **Étoile rouge** : Point où le F1-Score est maximal\n",
    "\n",
    "**Graphique 2 - F1-Score en fonction du seuil :**\n",
    "\n",
    "- **Axe X** : Seuil de décision (entre 0 et 1)\n",
    "- **Axe Y** : F1-Score correspondant\n",
    "- **Ligne orange** : Seuil par défaut (0.5)\n",
    "- **Note** : On garde le seuil 0.5 pour éviter l'optimisation excessive\n",
    "\n",
    "**Graphique 3 - Courbe de Calibration :**\n",
    "\n",
    "- **Ligne pointillée** : Calibration parfaite (si proba=0.3, 30% sont vraiment positifs)\n",
    "- **Courbe bleue** : Calibration du modèle\n",
    "- **Proche de la diagonale** = bonnes probabilités\n",
    "\n",
    "**Graphique 4 - Distribution des probabilités :**\n",
    "\n",
    "- **Vert** : Restés | **Rouge** : Partis\n",
    "- **Bonne séparation** = les deux distributions sont bien distinctes\n",
    "- **Chevauchement** = zone d'incertitude du modèle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3628d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest avec class_weight='balanced' pour analyse des probabilités\n",
    "print(\"Entraînement RF balanced pour analyse des probabilités...\")\n",
    "\n",
    "rf_pipeline = SkPipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            RandomForestClassifier(\n",
    "                n_estimators=100, class_weight=\"balanced\", random_state=42, n_jobs=-1\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Probabilités sur le test set\n",
    "y_test_proba = rf_pipeline.predict_proba(X_test)\n",
    "parti_idx = list(rf_pipeline.classes_).index(1)\n",
    "y_test_proba_parti = y_test_proba[:, parti_idx]\n",
    "y_test_binary = y_test.values\n",
    "\n",
    "print(f\"✅ RF entraîné - Classes: {rf_pipeline.classes_}\")\n",
    "\n",
    "# Analyse des déciles\n",
    "print(\"\\nANALYSE DES DÉCILES\")\n",
    "deciles = np.percentile(y_test_proba_parti, np.arange(0, 101, 10))\n",
    "for i, (low, high) in enumerate(zip(deciles[:-1], deciles[1:])):\n",
    "    mask = (y_test_proba_parti >= low) & (y_test_proba_parti < high)\n",
    "    if i == 9:\n",
    "        mask = (y_test_proba_parti >= low) & (y_test_proba_parti <= high)\n",
    "    n_obs = mask.sum()\n",
    "    n_partis = y_test_binary[mask].sum() if n_obs > 0 else 0\n",
    "    taux = (n_partis / n_obs * 100) if n_obs > 0 else 0\n",
    "    print(\n",
    "        f\"   D{i + 1:2d} [{low:.3f}-{high:.3f}]: {n_obs:3d} obs, {n_partis:2.0f} départs ({taux:5.1f}%)\"\n",
    "    )\n",
    "\n",
    "# Focus extrêmes\n",
    "top_10 = np.percentile(y_test_proba_parti, 90)\n",
    "bottom_10 = np.percentile(y_test_proba_parti, 10)\n",
    "print(\n",
    "    f\"\\nTop 10% (>= {top_10:.3f}): {(y_test_proba_parti >= top_10).sum()} empl, \"\n",
    "    f\"{y_test_binary[y_test_proba_parti >= top_10].sum():.0f} départs\"\n",
    ")\n",
    "print(\n",
    "    f\"Bottom 10% (<= {bottom_10:.3f}): {(y_test_proba_parti <= bottom_10).sum()} empl, \"\n",
    "    f\"{y_test_binary[y_test_proba_parti <= bottom_10].sum():.0f} départs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590fd22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23.3 Courbe Precision-Recall et Courbe de Calibration\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Courbe Precision-Recall\n",
    "precision_curve, recall_curve, thresholds = precision_recall_curve(\n",
    "    y_test_binary, y_test_proba_parti\n",
    ")\n",
    "ap_score = average_precision_score(y_test_binary, y_test_proba_parti)\n",
    "\n",
    "# Calcul du F1-score pour chaque seuil\n",
    "f1_scores = (\n",
    "    2\n",
    "    * (precision_curve[:-1] * recall_curve[:-1])\n",
    "    / (precision_curve[:-1] + recall_curve[:-1] + 1e-10)\n",
    ")\n",
    "best_threshold_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_threshold_idx]\n",
    "best_f1 = f1_scores[best_threshold_idx]\n",
    "best_precision = precision_curve[best_threshold_idx]\n",
    "best_recall = recall_curve[best_threshold_idx]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Graphique 1 : Courbe Precision-Recall\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(\n",
    "    recall_curve,\n",
    "    precision_curve,\n",
    "    \"b-\",\n",
    "    linewidth=2,\n",
    "    label=f\"PR Curve (AP = {ap_score:.3f})\",\n",
    ")\n",
    "ax1.scatter(\n",
    "    [best_recall],\n",
    "    [best_precision],\n",
    "    s=200,\n",
    "    c=\"red\",\n",
    "    marker=\"*\",\n",
    "    zorder=5,\n",
    "    label=f\"Max F1 @ seuil {best_threshold:.3f}\",\n",
    ")\n",
    "ax1.axhline(\n",
    "    y=y_test_binary.mean(), color=\"gray\", linestyle=\"--\", label=\"Baseline (random)\"\n",
    ")\n",
    "ax1.set_xlabel(\"Recall\", fontsize=12)\n",
    "ax1.set_ylabel(\"Precision\", fontsize=12)\n",
    "ax1.set_title(\"Courbe Precision-Recall\", fontsize=14, fontweight=\"bold\")\n",
    "ax1.legend(loc=\"best\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Graphique 2 : F1-Score en fonction du seuil\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(thresholds, f1_scores, \"g-\", linewidth=2)\n",
    "ax2.axvline(\n",
    "    x=best_threshold,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Max F1 = {best_f1:.3f}\",\n",
    ")\n",
    "ax2.scatter([best_threshold], [best_f1], s=200, c=\"red\", marker=\"*\", zorder=5)\n",
    "ax2.axvline(x=0.5, color=\"orange\", linestyle=\":\", label=\"Seuil par défaut (0.5)\")\n",
    "ax2.set_xlabel(\"Seuil de décision\", fontsize=12)\n",
    "ax2.set_ylabel(\"F1-Score\", fontsize=12)\n",
    "ax2.set_title(\"F1-Score en fonction du seuil\", fontsize=14, fontweight=\"bold\")\n",
    "ax2.legend(loc=\"best\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Graphique 3 : COURBE DE CALIBRATION\n",
    "ax3 = axes[1, 0]\n",
    "prob_true, prob_pred = calibration_curve(y_test_binary, y_test_proba_parti, n_bins=10)\n",
    "ax3.plot([0, 1], [0, 1], \"k--\", label=\"Calibration parfaite\")\n",
    "ax3.plot(prob_pred, prob_true, \"s-\", color=\"blue\", label=\"Random Forest\")\n",
    "ax3.set_xlabel(\"Probabilité moyenne prédite\", fontsize=12)\n",
    "ax3.set_ylabel(\"Fraction de positifs (réelle)\", fontsize=12)\n",
    "ax3.set_title(\"Courbe de Calibration\", fontsize=14, fontweight=\"bold\")\n",
    "ax3.legend(loc=\"best\")\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_xlim(0, 1)\n",
    "ax3.set_ylim(0, 1)\n",
    "\n",
    "# Graphique 4 : Distribution des probabilités\n",
    "ax4 = axes[1, 1]\n",
    "ax4.hist(\n",
    "    y_test_proba_parti[y_test_binary == 0],\n",
    "    bins=30,\n",
    "    alpha=0.5,\n",
    "    label=\"Restés\",\n",
    "    color=\"green\",\n",
    ")\n",
    "ax4.hist(\n",
    "    y_test_proba_parti[y_test_binary == 1],\n",
    "    bins=30,\n",
    "    alpha=0.5,\n",
    "    label=\"Partis\",\n",
    "    color=\"red\",\n",
    ")\n",
    "ax4.axvline(x=0.5, color=\"orange\", linestyle=\":\", linewidth=2, label=\"Seuil 0.5\")\n",
    "ax4.set_xlabel(\"Probabilité de départ\", fontsize=12)\n",
    "ax4.set_ylabel(\"Nombre d'employés\", fontsize=12)\n",
    "ax4.set_title(\n",
    "    \"Distribution des probabilités par classe\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "ax4.legend(loc=\"best\")\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Métriques avec seuil par défaut (0.5)\n",
    "print(\"MÉTRIQUES AVEC SEUIL PAR DÉFAUT (0.5)\")\n",
    "y_pred_default = (y_test_proba_parti >= 0.5).astype(int)\n",
    "print(f\"   Recall:    {recall_score(y_test_binary, y_pred_default):.3f}\")\n",
    "print(\n",
    "    f\"   Precision: {precision_score(y_test_binary, y_pred_default, zero_division=0):.3f}\"\n",
    ")\n",
    "print(f\"   F1-Score:  {f1_score(y_test_binary, y_pred_default):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a6df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion (seuil = 0.5)\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y_test_pred_default = y_pred_default  # Réutilisation\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "cm_default = confusion_matrix(y_test, y_test_pred_default, labels=[0, 1])\n",
    "ConfusionMatrixDisplay(cm_default, display_labels=[\"Resté\", \"Parti\"]).plot(\n",
    "    ax=ax, cmap=\"Blues\", values_format=\"d\"\n",
    ")\n",
    "ax.set_title(\"Matrice de Confusion (seuil = 0.5)\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Métriques\n",
    "tn, fp, fn, tp = cm_default.ravel()\n",
    "recall_val = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "precision_val = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "f1_val = (\n",
    "    2 * precision_val * recall_val / (precision_val + recall_val)\n",
    "    if (precision_val + recall_val) > 0\n",
    "    else 0\n",
    ")\n",
    "\n",
    "print(f\"TN: {tn} | FP: {fp} | FN: {fn} | TP: {tp}\")\n",
    "print(f\"Recall: {recall_val:.3f} | Precision: {precision_val:.3f} | F1: {f1_val:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e635763f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Synthèse Partie 4 : Gestion du déséquilibre des classes\n",
    "\n",
    "### Résultats comparatifs des techniques\n",
    "\n",
    "| Technique                          | Recall    | Precision | F1-Score  | ROC-AUC   |\n",
    "| ---------------------------------- | --------- | --------- | --------- | --------- |\n",
    "| **Logistic Regression (balanced)** | **0.743** | 0.378     | **0.501** | **0.830** |\n",
    "| Random Forest (balanced)           | 0.139     | 0.872     | 0.235     | 0.807     |\n",
    "| Random Forest + SMOTE              | 0.262     | 0.735     | 0.381     | 0.829     |\n",
    "| Random Forest + Undersampling      | 0.700     | 0.369     | 0.482     | 0.813     |\n",
    "\n",
    "**Meilleur modèle Partie 4** : **Logistic Regression avec class_weight='balanced'** (Recall=74.3%, F1=0.501, ROC-AUC=0.830)\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "1. **Logistic Regression > Random Forest** pour ce problème (meilleur Recall)\n",
    "2. **class_weight='balanced'** est la technique la plus simple et efficace\n",
    "3. **Undersampling** donne un excellent Recall (0.700) mais perd en Precision\n",
    "\n",
    "➡️ **Partie 5** : Optimisation avec LightGBM pour améliorer les performances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3188002a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Partie 5 : Fine-tuning et Interprétabilité (SHAP)\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "1. **Fine-tuning** : Optimiser les hyperparamètres avec GridSearchCV (CV stratifiée)\n",
    "2. **Modèle final** : LightGBM avec forte régularisation pour éviter l'overfitting\n",
    "3. **SHAP** : Interprétabilité globale (Beeswarm) et locale (Waterfall)\n",
    "\n",
    "### Pourquoi SHAP ?\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) est basé sur la **théorie des jeux** de Lloyd Shapley (Prix Nobel d'économie 2012).\n",
    "\n",
    "- **Mathématiquement juste** : Seule méthode satisfaisant toutes les propriétés d'équité\n",
    "- **Interprétable** : Comprendre l'impact de chaque feature sur la prédiction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f179d80a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 20. Imports et préparation Partie 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f15d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"Imports Partie 5 chargés (GridSearchCV, LightGBM, SHAP)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a15638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV sur LightGBM avec forte régularisation\n",
    "print(\"GRIDSEARCHCV - LightGBM avec régularisation\")\n",
    "\n",
    "pipeline_lgb = SkPipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", lgb.LGBMClassifier(random_state=42, verbose=-1, n_jobs=-1)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Grille avec forte régularisation pour éviter l'overfitting\n",
    "param_grid = {\n",
    "    \"classifier__n_estimators\": [30, 50, 75],\n",
    "    \"classifier__max_depth\": [2, 3],\n",
    "    \"classifier__learning_rate\": [0.01, 0.03, 0.05],\n",
    "    \"classifier__class_weight\": [\"balanced\"],\n",
    "    \"classifier__num_leaves\": [4, 7, 10],\n",
    "    \"classifier__min_child_samples\": [50, 75, 100],\n",
    "    \"classifier__reg_alpha\": [1.0, 5.0, 10.0],\n",
    "    \"classifier__reg_lambda\": [1.0, 5.0, 10.0],\n",
    "    \"classifier__subsample\": [0.7, 0.8],\n",
    "    \"classifier__colsample_bytree\": [0.7, 0.8],\n",
    "}\n",
    "\n",
    "f1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline_lgb,\n",
    "    param_grid,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring=f1_scorer,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nMeilleurs paramètres:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"   {param}: {value}\")\n",
    "print(f\"\\nMeilleur F1 (CV): {grid_search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1f0e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation du meilleur modèle sur le test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_proba_optimized = best_model.predict_proba(X_test)[:, 1]\n",
    "y_pred_optimized = (y_proba_optimized >= 0.5).astype(int)\n",
    "\n",
    "print(\"MÉTRIQUES SUR LE JEU DE TEST:\")\n",
    "print(f\"   Recall:     {recall_score(y_test, y_pred_optimized, pos_label=1):.3f}\")\n",
    "print(\n",
    "    f\"   Precision:  {precision_score(y_test, y_pred_optimized, pos_label=1, zero_division=0):.3f}\"\n",
    ")\n",
    "print(f\"   F1-Score:   {f1_score(y_test, y_pred_optimized, pos_label=1):.3f}\")\n",
    "print(f\"   ROC-AUC:    {roc_auc_score(y_test, y_proba_optimized):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3699de05",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 21. Interprétabilité avec SHAP\n",
    "\n",
    "### Pourquoi SHAP ?\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) est basé sur les **valeurs de Shapley** de la théorie des jeux :\n",
    "\n",
    "- **Mathématiquement juste** : Seule méthode qui respecte toutes les propriétés d'équité (efficacité, symétrie, linéarité, nullité)\n",
    "- **Interprétable** : Chaque feature reçoit une \"contribution\" à la prédiction\n",
    "- **Global et local** : Permet de comprendre le modèle dans son ensemble ET chaque prédiction individuelle\n",
    "\n",
    "### Types d'analyses\n",
    "\n",
    "1. **Beeswarm Plot** (global) : Vue d'ensemble de l'importance de chaque feature\n",
    "2. **Permutation Importance** (global) : Comparaison avec méthode sklearn\n",
    "3. **Waterfall Plot** (local) : Explication d'une prédiction individuelle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc00a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer le preprocessor et le classifier du meilleur modèle\n",
    "preprocessor_fitted = best_model.named_steps[\"preprocessor\"]\n",
    "classifier_fitted = best_model.named_steps[\"classifier\"]\n",
    "\n",
    "# Transformer les données de test\n",
    "X_test_transformed = preprocessor_fitted.transform(X_test)\n",
    "\n",
    "# Récupérer les noms des features après transformation\n",
    "# Note: get_feature_names_out() sans argument utilise les noms appris lors du fit\n",
    "cat_feature_names = (\n",
    "    preprocessor_fitted.named_transformers_[\"cat\"].get_feature_names_out().tolist()\n",
    ")\n",
    "num_feature_names = (\n",
    "    num_cols.copy()\n",
    ")  # num_cols est définie dans le preprocessing (23 colonnes)\n",
    "all_feature_names = num_feature_names + cat_feature_names\n",
    "\n",
    "print(f\"Nombre de features après transformation: {len(all_feature_names)}\")\n",
    "print(f\"   - Features numériques: {len(num_feature_names)}\")\n",
    "print(f\"   - Features catégorielles (après OHE): {len(cat_feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067a510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calcul des SHAP values (TreeExplainer pour LightGBM)...\")\n",
    "\n",
    "# TreeExplainer est optimisé pour les modèles à base d'arbres\n",
    "explainer = shap.TreeExplainer(classifier_fitted)\n",
    "shap_values = explainer.shap_values(X_test_transformed)\n",
    "\n",
    "# Pour classification binaire, shap_values peut être une liste [classe_0, classe_1]\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values = shap_values[1]  # On prend la classe positive (Parti)\n",
    "\n",
    "print(\"SHAP values calculées\")\n",
    "print(f\"   Shape: {shap_values.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681ce598",
   "metadata": {},
   "source": [
    "### 21.1 Feature Importance Globale - Beeswarm Plot\n",
    "\n",
    "Le **Beeswarm Plot** montre :\n",
    "\n",
    "- **Axe Y** : Les features triées par importance\n",
    "- **Axe X** : L'impact sur la prédiction (SHAP value)\n",
    "- **Couleur** : La valeur de la feature (rouge = élevée, bleu = basse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5eee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Beeswarm Plot - Importance globale des features\n",
    "X_test_df = pd.DataFrame(X_test_transformed, columns=all_feature_names)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "shap.summary_plot(shap_values, X_test_df, plot_type=\"dot\", show=False, max_display=20)\n",
    "plt.title(\n",
    "    \"SHAP - Impact des features sur la probabilité de départ\",\n",
    "    fontsize=12,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1f4017",
   "metadata": {},
   "source": [
    "### 21.2 Feature Importance Locale - Waterfall Plot\n",
    "\n",
    "Le **Waterfall Plot** explique UNE prédiction individuelle :\n",
    "\n",
    "- Comment chaque feature a contribué à passer de la valeur de base (moyenne) à la prédiction finale\n",
    "- Utile pour expliquer une décision à un manager RH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daade1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des exemples pour waterfall plots\n",
    "y_pred_test = y_pred_optimized\n",
    "true_positives = np.where((y_test == 1) & (y_pred_test == 1))[0]\n",
    "true_negatives = np.where((y_test == 0) & (y_pred_test == 0))[0]\n",
    "\n",
    "print(f\"Vrais Positifs: {len(true_positives)} | Vrais Négatifs: {len(true_negatives)}\")\n",
    "\n",
    "# Objet Explanation pour waterfall\n",
    "explanation = shap.Explanation(\n",
    "    values=shap_values,\n",
    "    base_values=np.full(\n",
    "        len(shap_values),\n",
    "        explainer.expected_value\n",
    "        if not hasattr(explainer.expected_value, \"__len__\")\n",
    "        else explainer.expected_value[1],\n",
    "    ),\n",
    "    data=X_test_transformed,\n",
    "    feature_names=all_feature_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb4f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waterfall Plot - Exemple d'un employé \"Parti\"\n",
    "if len(true_positives) > 0:\n",
    "    idx_parti = true_positives[0]\n",
    "    proba_parti = y_proba_optimized[idx_parti]\n",
    "\n",
    "    print(f\"Employé PARTI - Index: {idx_parti}, Proba: {proba_parti:.3f}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.plots.waterfall(explanation[idx_parti], max_display=15, show=False)\n",
    "    plt.title(\n",
    "        f\"Waterfall - Employé Parti (proba={proba_parti:.3f})\",\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Aucun vrai positif trouvé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a30c2d0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Synthèse Partie 5 : Fine-tuning et Interprétabilité\n",
    "\n",
    "### Modèle final : LightGBM avec GridSearchCV (régularisé)\n",
    "\n",
    "| Métrique      | Valeur   | Interprétation                                      |\n",
    "|---------------|----------|-----------------------------------------------------|\n",
    "| **Recall**    | **0.596**| Détecte 28 départs sur 47 (59.6%)                   |\n",
    "| **Precision** | 0.368    | 28 alertes correctes sur 76 levées                  |\n",
    "| **F1-Score**  | **0.455**| Meilleur compromis recall/precision                 |\n",
    "| **ROC-AUC**   | 0.800    | Bonne capacité de discrimination                    |\n",
    "\n",
    "### 🔧 Meilleurs hyperparamètres (GridSearchCV)\n",
    "\n",
    "```\n",
    "learning_rate: 0.05       # Apprentissage lent → généralisation\n",
    "max_depth: 3              # Arbres peu profonds → évite overfitting\n",
    "n_estimators: 75          # Nombre d'arbres modéré\n",
    "num_leaves: 10            # Peu de feuilles → modèle simple\n",
    "min_child_samples: 50     # Minimum 50 samples/feuille → robustesse\n",
    "reg_alpha: 1.0            # Régularisation L1\n",
    "reg_lambda: 1.0           # Régularisation L2  \n",
    "subsample: 0.7            # Bagging 70% → réduit variance\n",
    "colsample_bytree: 0.7     # Feature sampling 70%\n",
    "class_weight: balanced    # Gestion du déséquilibre\n",
    "```\n",
    "\n",
    "### ✅ Stratégie anti-overfitting\n",
    "\n",
    "Le modèle utilise une **forte régularisation** pour éviter l'overfitting observé avec Random Forest :\n",
    "\n",
    "| Technique | Effet |\n",
    "|-----------|-------|\n",
    "| `max_depth=3` | Arbres superficiels, pas de mémorisation |\n",
    "| `reg_alpha/lambda=1.0` | Pénalise les poids extrêmes |\n",
    "| `subsample=0.7` | Bagging pour réduire la variance |\n",
    "| `min_child_samples=50` | Évite les feuilles avec peu d'exemples |\n",
    "\n",
    "**Résultat** : Le F1 en cross-validation (0.534) est proche du F1 sur test (0.455), ce qui indique une bonne généralisation.\n",
    "\n",
    "### SHAP - Top 5 Features les plus impactantes\n",
    "\n",
    "1. **heures_supplementaires_Oui** - Faire des heures sup **augmente fortement** le risque de départ\n",
    "2. **nombre_participation_pee** - Faible participation au PEE → risque accru\n",
    "3. **satisfaction_globale** - Faible satisfaction globale → risque accru\n",
    "4. **age** - Employés plus jeunes → risque accru de départ\n",
    "5. **revenu_mensuel** - Un salaire **bas** augmente le risque de départ\n",
    "\n",
    "### Exemple d'interprétation locale (SHAP Waterfall)\n",
    "\n",
    "L'employé prédit comme \"Parti\" montre :\n",
    "- **Heures supplémentaires** : facteur majeur d'augmentation du risque\n",
    "- **Jeune âge** : contribue au risque\n",
    "- **Nombreuses expériences précédentes** : signe d'instabilité\n",
    "- **Participation au PEE** : facteur de rétention\n",
    "\n",
    "### Recommandations métier pour TechNova Partners\n",
    "\n",
    "| Priorité | Action | Impact attendu |\n",
    "|----------|--------|----------------|\n",
    "| 🔴 Haute | **Limiter les heures supplémentaires** récurrentes | Réduit le facteur #1 de départ |\n",
    "| 🔴 Haute | **Réviser les grilles salariales** (employés sous-payés) | Améliore la rétention |\n",
    "| 🟠 Moyenne | Proposer **télétravail/horaires flexibles** | Équilibre vie pro/perso |\n",
    "| 🟠 Moyenne | **Promouvoir le PEE** (Plan Épargne Entreprise) | Engagement financier |\n",
    "| 🟡 Basse | **Programmes de mentorat** pour les jeunes talents | Fidélisation des juniors |\n",
    "\n",
    "### Conclusion générale\n",
    "\n",
    "Le modèle **LightGBM régularisé** permet de :\n",
    "- ✅ **Identifier** les employés à risque avec un **Recall de 59.6%** (détecte 28 départs sur 47)\n",
    "- ✅ **Comprendre** les facteurs de départ grâce à **SHAP** (heures sup, PEE, satisfaction, âge)\n",
    "- ✅ **Prioriser** les actions RH avec des **probabilités calibrées** (10% à 90%)\n",
    "- ✅ **Généraliser** correctement grâce à la régularisation\n",
    "\n",
    "### ⚠️ Limites et améliorations possibles\n",
    "\n",
    "| Limite | Amélioration potentielle |\n",
    "|--------|--------------------------|\n",
    "| Recall de 60% = 40% de départs non détectés | Collecter plus de données |\n",
    "| Dataset de 1176 employés | Ajouter données longitudinales |\n",
    "| Features SIRH uniquement | Intégrer feedback entretiens, engagement teams |\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
