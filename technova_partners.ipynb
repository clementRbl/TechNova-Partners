{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4086c666",
   "metadata": {},
   "source": [
    "# TechNova Partners - Analyse du Churn RH\n",
    "\n",
    "**Projet :** Identification des causes de démission et modélisation prédictive  \n",
    "**Client :** TechNova Partners (ESN spécialisée en transformation digitale)\n",
    "\n",
    "---\n",
    "\n",
    "## Contexte du Projet\n",
    "\n",
    "TechNova Partners fait face à un turnover élevé. L'objectif est de :\n",
    "\n",
    "1. **Analyser** les données RH pour identifier les différences entre employés partis et restés\n",
    "2. **Construire** un modèle de classification pour prédire les démissions\n",
    "3. **Extraire** les causes potentielles via l'interprétation du modèle (SHAP)\n",
    "\n",
    "**Sources de données :**\n",
    "\n",
    "- `data/extrait_sirh.csv` - Informations RH (âge, salaire, poste, ancienneté...)\n",
    "- `data/extrait_eval.csv` - Évaluations de performance\n",
    "- `data/extrait_sondage.csv` - Sondage employés + **variable cible**\n",
    "\n",
    "---\n",
    "\n",
    "## Structure du Notebook\n",
    "\n",
    "**Partie 1 : Exploration des Données**\n",
    "\n",
    "- Chargement et compréhension des fichiers\n",
    "- Fusion et création du dataset central\n",
    "- Analyse exploratoire et visualisations\n",
    "\n",
    "**Partie 2 : Feature Engineering**\n",
    "\n",
    "- Préparation des features (X)\n",
    "- Encodage des variables catégorielles\n",
    "- Gestion des corrélations\n",
    "\n",
    "**Partie 3 : Modélisation Baseline**\n",
    "\n",
    "- Modèle Dummy (référence)\n",
    "- Modèle linéaire\n",
    "- Modèle non-linéaire (arbre)\n",
    "\n",
    "**Partie 4 : Gestion du Déséquilibre**\n",
    "\n",
    "- Stratification\n",
    "- Class weights / Undersampling / Oversampling (SMOTE)\n",
    "- Calibration de probabilité\n",
    "- Validation croisée stratifiée\n",
    "\n",
    "**Partie 5 : Optimisation et Interpretation**\n",
    "\n",
    "- Fine-tuning des hyperparamètres\n",
    "- Feature importance globale (SHAP, Permutation)\n",
    "- Feature importance locale (SHAP Waterfall)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1a5ad0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Importation des librairies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dba98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings  # noqa: E402\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306b0aaf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Chargement des données\n",
    "\n",
    "Chargement des 3 fichiers CSV et examen de structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a584d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sirh = pd.read_csv(\"data/extrait_sirh.csv\")\n",
    "df_eval = pd.read_csv(\"data/extrait_eval.csv\")\n",
    "df_sondage = pd.read_csv(\"data/extrait_sondage.csv\")\n",
    "\n",
    "print(f\"Fichier SIRH : {df_sirh.shape[0]} lignes, {df_sirh.shape[1]} colonnes\")\n",
    "print(f\"Fichier Évaluations : {df_eval.shape[0]} lignes, {df_eval.shape[1]} colonnes\")\n",
    "print(f\"Fichier Sondage : {df_sondage.shape[0]} lignes, {df_sondage.shape[1]} colonnes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7859bfec",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Exploration initiale de chaque fichier\n",
    "\n",
    "Avant de fusionner, comprenons le contenu et la structure de chaque fichier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0cd45c",
   "metadata": {},
   "source": [
    "### 3.1 Fichier SIRH (extrait_sirh.csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52f0021",
   "metadata": {},
   "source": [
    "#### Aperçu des premières lignes\n",
    "\n",
    "Visualisons les premières lignes pour comprendre la structure et le contenu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dff1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sirh.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1555732",
   "metadata": {},
   "source": [
    "#### Structure et types de données\n",
    "\n",
    "Analysons les types de colonnes, la mémoire utilisée et les valeurs non-nulles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0529cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sirh.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a461b88d",
   "metadata": {},
   "source": [
    "#### Statistiques descriptives\n",
    "\n",
    "Calculons les statistiques de base (moyenne, écart-type, min, max, quartiles) pour les variables numériques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8b7892",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sirh.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5ab8be",
   "metadata": {},
   "source": [
    "#### Analyse des variables catégorielles\n",
    "\n",
    "Examinons les valeurs uniques et leur fréquence pour chaque variable catégorielle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf3e75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valeurs uniques des colonnes catégorielles SIRH :\")\n",
    "for col in df_sirh.select_dtypes(include=\"object\").columns:\n",
    "    print(f\"\\n{col}: {df_sirh[col].nunique()} valeurs uniques\")\n",
    "    print(df_sirh[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79318fd",
   "metadata": {},
   "source": [
    "### 3.2 Fichier Évaluations (extrait_eval.csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11411bc9",
   "metadata": {},
   "source": [
    "#### Aperçu des premières lignes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771a93d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6a667e",
   "metadata": {},
   "source": [
    "#### Structure et types de données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16286df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c928a9b5",
   "metadata": {},
   "source": [
    "#### Statistiques descriptives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837aef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistiques descriptives Evaluations (variables numeriques) :\")\n",
    "df_eval.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca31359",
   "metadata": {},
   "source": [
    "#### Analyse des variables catégorielles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d90ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valeurs uniques des colonnes catégorielles\n",
    "print(\"Valeurs uniques des colonnes catégorielles Evaluations :\")\n",
    "for col in df_eval.select_dtypes(include=\"object\").columns:\n",
    "    print(f\"\\n{col}: {df_eval[col].nunique()} valeurs uniques\")\n",
    "    print(df_eval[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d33eff",
   "metadata": {},
   "source": [
    "### 3.3 Fichier Sondage (extrait_sondage.csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400e8e4d",
   "metadata": {},
   "source": [
    "#### Aperçu des premières lignes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b17c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sondage.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c403c29a",
   "metadata": {},
   "source": [
    "#### Structure et types de données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d7ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sondage.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5579a95a",
   "metadata": {},
   "source": [
    "#### Statistiques descriptives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb217995",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sondage.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a13eaf",
   "metadata": {},
   "source": [
    "#### Analyse des variables catégorielles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b9cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_sondage.select_dtypes(include=\"object\").columns:\n",
    "    print(f\"\\n{col}: {df_sondage[col].nunique()} valeurs uniques\")\n",
    "    print(df_sondage[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8635855",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Identification des clés de jointure\n",
    "\n",
    "Pour fusionner les 3 fichiers, nous devons identifier les colonnes qui permettent de faire le lien entre eux.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a67bf0e",
   "metadata": {},
   "source": [
    "#### Analyse des colonnes identifiantes\n",
    "\n",
    "Examinons les colonnes qui nous permettront de faire les jointures entre fichiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42d5f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Colonnes SIRH :\")\n",
    "print(df_sirh.columns.tolist())\n",
    "print(\n",
    "    f\"\\nClé potentielle 'id_employee' : {df_sirh['id_employee'].nunique()} valeurs uniques sur {len(df_sirh)} lignes\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"\\nColonnes Évaluations :\")\n",
    "print(df_eval.columns.tolist())\n",
    "print(\n",
    "    f\"\\nClé potentielle 'eval_number' : {df_eval['eval_number'].nunique()} valeurs uniques sur {len(df_eval)} lignes\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"\\nColonnes Sondage :\")\n",
    "print(df_sondage.columns.tolist())\n",
    "print(\n",
    "    f\"\\nClé potentielle 'code_sondage' : {df_sondage['code_sondage'].nunique()} valeurs uniques sur {len(df_sondage)} lignes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9872483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysons le format des clés pour comprendre comment les relier\n",
    "print(\"Exemples de clés :\")\n",
    "print(f\"\\nSIRH - id_employee (premiers) : {df_sirh['id_employee'].head(10).tolist()}\")\n",
    "print(f\"\\nEval - eval_number (premiers) : {df_eval['eval_number'].head(10).tolist()}\")\n",
    "print(\n",
    "    f\"\\nSondage - code_sondage (premiers) : {df_sondage['code_sondage'].head(10).tolist()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a199f363",
   "metadata": {},
   "source": [
    "#### Analyse du format des clés\n",
    "\n",
    "Regardons de plus près comment sont structurées ces clés.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a85a308",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Comparaison du nombre de lignes :\")\n",
    "print(f\"  - SIRH : {len(df_sirh)} lignes\")\n",
    "print(f\"  - Évaluations : {len(df_eval)} lignes\")\n",
    "print(f\"  - Sondage : {len(df_sondage)} lignes\")\n",
    "\n",
    "# Si tous les fichiers ont le même nombre de lignes,\n",
    "# ils correspondent probablement aux mêmes employés"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2828de03",
   "metadata": {},
   "source": [
    "#### Vérification de la cohérence des données\n",
    "\n",
    "Comparons le nombre de lignes pour détecter d'éventuels problèmes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b40ef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analyse de l'ordre et de la correspondance des lignes :\\n\")\n",
    "\n",
    "# Hypothèse : si tous ont le même nombre de lignes dans le même ordre,\n",
    "# les employés sont peut-être déjà alignés par index\n",
    "\n",
    "# Vérifions s'il y a des colonnes communes\n",
    "print(\"Colonnes communes entre les fichiers :\")\n",
    "sirh_cols = set(df_sirh.columns)\n",
    "eval_cols = set(df_eval.columns)\n",
    "sondage_cols = set(df_sondage.columns)\n",
    "\n",
    "print(f\"\\nSIRH ∩ Évaluations : {sirh_cols.intersection(eval_cols)}\")\n",
    "print(f\"SIRH ∩ Sondage : {sirh_cols.intersection(sondage_cols)}\")\n",
    "print(f\"Évaluations ∩ Sondage : {eval_cols.intersection(sondage_cols)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed2fdca",
   "metadata": {},
   "source": [
    "#### Conclusion : Fusion par index\n",
    "\n",
    "**Résultat attendu :** Aucune colonne commune (set() vide partout).\n",
    "\n",
    "**Pourquoi ?** Chaque fichier a des colonnes **uniques** :\n",
    "\n",
    "- SIRH → infos administratives (id_employee, ancienneté, salaire...)\n",
    "- Évaluations → métriques performance (eval_number, satisfaction_manager, note_autonomie...)\n",
    "- Sondage → perception employé (code_sondage, equilibre_vie_pro_perso, niveau_stress...)\n",
    "\n",
    "**Stratégie de fusion :** Comme les 3 fichiers ont le **même nombre de lignes (1470)** et que les indices correspondent (id_employee=1 ↔ eval_number=\"E_1\" ↔ code_sondage=1), nous pouvons effectuer une **concaténation horizontale par index** avec `pd.concat(..., axis=1)`.\n",
    "\n",
    "Aucune jointure SQL n'est nécessaire, Les lignes sont déjà alignées.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d422a52",
   "metadata": {},
   "source": [
    "#### Recherche de colonnes communes\n",
    "\n",
    "Vérifions s'il existe des colonnes partagées entre les fichiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30d218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysons la structure des colonnes \"clés\"\n",
    "print(\"Analyse détaillée des colonnes identifiantes :\\n\")\n",
    "\n",
    "# SIRH\n",
    "print(\"FICHIER SIRH - Colonne 'id_employee'\")\n",
    "print(f\"Type de données : {df_sirh['id_employee'].dtype}\")\n",
    "print(f\"Valeurs uniques : {df_sirh['id_employee'].nunique()}\")\n",
    "print(f\"Total de lignes : {len(df_sirh)}\")\n",
    "print(f\"Valeurs manquantes : {df_sirh['id_employee'].isna().sum()}\")\n",
    "print(f\"Doublons : {df_sirh['id_employee'].duplicated().sum()}\")\n",
    "print(f\"\\nExemples : {df_sirh['id_employee'].head(5).tolist()}\")\n",
    "\n",
    "# Évaluations\n",
    "print(\"FICHIER ÉVALUATIONS - Colonne 'eval_number'\")\n",
    "print(f\"Type de données : {df_eval['eval_number'].dtype}\")\n",
    "print(f\"Valeurs uniques : {df_eval['eval_number'].nunique()}\")\n",
    "print(f\"Total de lignes : {len(df_eval)}\")\n",
    "print(f\"Valeurs manquantes : {df_eval['eval_number'].isna().sum()}\")\n",
    "print(f\"Doublons : {df_eval['eval_number'].duplicated().sum()}\")\n",
    "print(f\"\\nExemples : {df_eval['eval_number'].head(5).tolist()}\")\n",
    "\n",
    "# Sondage\n",
    "print(\"FICHIER SONDAGE - Colonne 'code_sondage'\")\n",
    "print(f\"Type de données : {df_sondage['code_sondage'].dtype}\")\n",
    "print(f\"Valeurs uniques : {df_sondage['code_sondage'].nunique()}\")\n",
    "print(f\"Total de lignes : {len(df_sondage)}\")\n",
    "print(f\"Valeurs manquantes : {df_sondage['code_sondage'].isna().sum()}\")\n",
    "print(f\"Doublons : {df_sondage['code_sondage'].duplicated().sum()}\")\n",
    "print(f\"\\nExemples : {df_sondage['code_sondage'].head(5).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb4029",
   "metadata": {},
   "source": [
    "#### Analyse détaillée des clés de jointure\n",
    "\n",
    "Explorons en détail chaque colonne identifiante pour comprendre leur structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dcf30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifions si les clés peuvent être liées entre elles\n",
    "print(\"Recherche de correspondances potentielles entre les clés :\\n\")\n",
    "\n",
    "# Testons si une partie de la clé correspond entre fichiers\n",
    "# Par exemple, si id_employee = \"EMP_001\" et eval_number contient \"001\"\n",
    "\n",
    "print(\"Premières valeurs de chaque clé pour comparaison visuelle :\")\n",
    "comparison_df = pd.DataFrame(\n",
    "    {\n",
    "        \"id_employee\": df_sirh[\"id_employee\"].head(10),\n",
    "        \"eval_number\": df_eval[\"eval_number\"].head(10),\n",
    "        \"code_sondage\": df_sondage[\"code_sondage\"].head(10),\n",
    "    }\n",
    ")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831a01f1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Fusion des données\n",
    "\n",
    "Les 3 fichiers ont le même nombre de lignes (1470) et les clés correspondent (id*employee = code_sondage, eval_number = \"E*\" + id).\n",
    "Nous allons les fusionner par concaténation horizontale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b6ff60",
   "metadata": {},
   "source": [
    "#### Création du DataFrame central\n",
    "\n",
    "Fusion des 3 fichiers par concaténation horizontale (axis=1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b5a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.concat([df_sirh, df_eval, df_sondage], axis=1)\n",
    "\n",
    "print(\"DataFrame fusionné créé :\")\n",
    "print(f\"  - {df_merged.shape[0]} lignes\")\n",
    "print(f\"  - {df_merged.shape[1]} colonnes\")\n",
    "print(f\"\\nColonnes : {df_merged.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0834aa13",
   "metadata": {},
   "source": [
    "#### Gestion des colonnes dupliquées\n",
    "\n",
    "Vérification et suppression des éventuelles colonnes en double.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ec4637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification des colonnes dupliquées\n",
    "duplicated_cols = df_merged.columns[df_merged.columns.duplicated()].tolist()\n",
    "\n",
    "if duplicated_cols:\n",
    "    print(f\"{len(duplicated_cols)} colonne(s) dupliquée(s) détectée(s) :\")\n",
    "    for col in duplicated_cols:\n",
    "        print(f\"  - {col}\")\n",
    "\n",
    "    # Suppression des doublons (on garde la première occurrence)\n",
    "    df_merged = df_merged.loc[:, ~df_merged.columns.duplicated()]\n",
    "    print(\"\\nColonnes dupliquées supprimées\")\n",
    "    print(f\"Nouvelles dimensions : {df_merged.shape}\")\n",
    "else:\n",
    "    print(\"Aucune colonne dupliquée détectée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca5fca7",
   "metadata": {},
   "source": [
    "#### Aperçu du DataFrame central\n",
    "\n",
    "Visualisation des premières lignes du dataset fusionné.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b7fd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05128d7c",
   "metadata": {},
   "source": [
    "#### Structure du DataFrame fusionné\n",
    "\n",
    "Informations sur les types de données et la mémoire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a48f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367a229f",
   "metadata": {},
   "source": [
    "#### Analyse de la variable cible\n",
    "\n",
    "Distribution de `a_quitte_l_entreprise` - la variable à prédire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9daf702",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Variable cible - 'a_quitte_l_entreprise' :\")\n",
    "print(f\"Type : {df_merged['a_quitte_l_entreprise'].dtype}\")\n",
    "print(\"\\nDistribution :\")\n",
    "print(df_merged[\"a_quitte_l_entreprise\"].value_counts())\n",
    "print(\"\\nProportions (%) :\")\n",
    "print((df_merged[\"a_quitte_l_entreprise\"].value_counts(normalize=True) * 100).round(2))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Comptage\n",
    "df_merged[\"a_quitte_l_entreprise\"].value_counts().plot(\n",
    "    kind=\"bar\", ax=ax[0], color=[\"#2ecc71\", \"#e74c3c\"]\n",
    ")\n",
    "ax[0].set_title(\"Distribution de la variable cible\", fontsize=12, fontweight=\"bold\")\n",
    "ax[0].set_xlabel(\"A quitté l'entreprise\")\n",
    "ax[0].set_ylabel(\"Nombre d'employés\")\n",
    "ax[0].set_xticklabels([\"Non\", \"Oui\"], rotation=0)\n",
    "\n",
    "# Proportions\n",
    "df_merged[\"a_quitte_l_entreprise\"].value_counts().plot(\n",
    "    kind=\"pie\",\n",
    "    ax=ax[1],\n",
    "    autopct=\"%1.1f%%\",\n",
    "    colors=[\"#2ecc71\", \"#e74c3c\"],\n",
    "    labels=[\"Restés\", \"Partis\"],\n",
    ")\n",
    "ax[1].set_title(\"Proportions\", fontsize=12, fontweight=\"bold\")\n",
    "ax[1].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"OBSERVATION CRITIQUE : Déséquilibre des classes !\")\n",
    "print(\"     → À gérer en modélisation (stratification, class_weights, SMOTE)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab20774",
   "metadata": {},
   "source": [
    "#### Synthèse : Variable cible\n",
    "\n",
    "- **84% restés** vs **16% partis** → Ratio 5:1\n",
    "- Déséquilibre à gérer : stratification, class_weight, resampling, calibration\n",
    "- Accuracy insuffisante comme métrique (84% sans rien faire)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebb775a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Vue d'ensemble du dataset central\n",
    "\n",
    "Avant de comparer les employés partis vs restés, vérifions la qualité et la structure des données :\n",
    "\n",
    "- Valeurs manquantes\n",
    "- Types de colonnes (numériques vs catégorielles)\n",
    "- Colonnes identifiantes à exclure de l'analyse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db38ef20",
   "metadata": {},
   "source": [
    "#### Analyse des valeurs manquantes\n",
    "\n",
    "Vérifions s'il y a des données manquantes dans le dataset fusionné.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f54449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des valeurs manquantes\n",
    "print(\"Analyse des valeurs manquantes :\\n\")\n",
    "\n",
    "missing_values = df_merged.isnull().sum()\n",
    "missing_pct = (df_merged.isnull().sum() / len(df_merged) * 100).round(2)\n",
    "\n",
    "missing_df = pd.DataFrame(\n",
    "    {\"Valeurs manquantes\": missing_values, \"Pourcentage (%)\": missing_pct}\n",
    ")\n",
    "\n",
    "# Afficher seulement les colonnes avec des valeurs manquantes\n",
    "missing_with_values = missing_df[missing_df[\"Valeurs manquantes\"] > 0]\n",
    "\n",
    "if len(missing_with_values) > 0:\n",
    "    print(f\"{len(missing_with_values)} colonne(s) avec des valeurs manquantes :\")\n",
    "    print(missing_with_values.sort_values(\"Pourcentage (%)\", ascending=False))\n",
    "else:\n",
    "    print(\"Aucune valeur manquante dans le dataset !\")\n",
    "    print(f\"   → {df_merged.shape[0]} lignes × {df_merged.shape[1]} colonnes complètes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2018e0d7",
   "metadata": {},
   "source": [
    "#### Classification des colonnes par type\n",
    "\n",
    "Identifions les colonnes numériques et catégorielles pour orienter l'analyse exploratoire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f358b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification des colonnes par type :\\n\")\n",
    "\n",
    "# Colonnes identifiantes (à exclure de l'analyse)\n",
    "id_cols = [\"id_employee\", \"eval_number\", \"code_sondage\"]\n",
    "\n",
    "# Variable cible\n",
    "target_col = \"a_quitte_l_entreprise\"\n",
    "\n",
    "# Colonnes numériques (excluant les IDs)\n",
    "numeric_cols = df_merged.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "numeric_cols = [col for col in numeric_cols if col not in id_cols + [target_col]]\n",
    "\n",
    "# Colonnes catégorielles\n",
    "categorical_cols = df_merged.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "categorical_cols = [\n",
    "    col for col in categorical_cols if col not in id_cols + [target_col]\n",
    "]\n",
    "\n",
    "print(f\"Colonnes identifiantes ({len(id_cols)}) - À EXCLURE :\")\n",
    "print(f\"   {id_cols}\\n\")\n",
    "\n",
    "print(\"Variable cible :\")\n",
    "print(f\"   {target_col}\\n\")\n",
    "\n",
    "print(f\"Colonnes numériques ({len(numeric_cols)}) :\")\n",
    "print(f\"   {numeric_cols}\\n\")\n",
    "\n",
    "print(f\"Colonnes catégorielles ({len(categorical_cols)}) :\")\n",
    "print(f\"   {categorical_cols}\")\n",
    "\n",
    "print(f\"Total features analysables : {len(numeric_cols) + len(categorical_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3001d5e",
   "metadata": {},
   "source": [
    "#### Résumé structuré du dataset\n",
    "\n",
    "Tableau récapitulatif avec le type, les valeurs uniques et des exemples pour chaque colonne.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f860ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = []\n",
    "\n",
    "for col in df_merged.columns:\n",
    "    if col in id_cols:\n",
    "        category = \"Identifiant\"\n",
    "    elif col == target_col:\n",
    "        category = \"Cible\"\n",
    "    elif col in numeric_cols:\n",
    "        category = \"Numérique\"\n",
    "    else:\n",
    "        category = \"Catégorielle\"\n",
    "\n",
    "    summary_data.append(\n",
    "        {\n",
    "            \"Colonne\": col,\n",
    "            \"Catégorie\": category,\n",
    "            \"Type\": str(df_merged[col].dtype),\n",
    "            \"Valeurs uniques\": df_merged[col].nunique(),\n",
    "            \"Exemple\": str(df_merged[col].iloc[0])[:30],\n",
    "        }\n",
    "    )\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df6fd0b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Analyse exploratoire comparative : Partis vs Restés\n",
    "\n",
    "Objectif principal de cette section : **identifier les différences clés** entre les employés ayant quitté l'entreprise et ceux qui y sont restés.\n",
    "\n",
    "Nous utiliserons **Plotly** pour des graphiques interactifs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2343e5e7",
   "metadata": {},
   "source": [
    "#### Import de Plotly et préparation des données\n",
    "\n",
    "Configuration de Plotly et création d'une colonne lisible pour la variable cible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c74251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "df_merged[\"statut\"] = df_merged[\"a_quitte_l_entreprise\"].map(\n",
    "    {\"Oui\": \"Parti\", \"Non\": \"Resté\"}\n",
    ")\n",
    "\n",
    "colors = {\"Resté\": \"#2ecc71\", \"Parti\": \"#e74c3c\"}\n",
    "\n",
    "print(f\"   Distribution : {df_merged['statut'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356fc251",
   "metadata": {},
   "source": [
    "### 7.1 Analyse des variables numériques\n",
    "\n",
    "Comparons les **moyennes** des variables numériques entre les employés partis et restés avec un graphique unique et lisible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ad407",
   "metadata": {},
   "source": [
    "#### Différences relatives des moyennes (Partis vs Restés)\n",
    "\n",
    "Un seul graphique montrant la différence en pourcentage pour chaque variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21103a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des moyennes par statut\n",
    "means_parti = df_merged[df_merged[\"statut\"] == \"Parti\"][numeric_cols].mean()\n",
    "means_reste = df_merged[df_merged[\"statut\"] == \"Resté\"][numeric_cols].mean()\n",
    "\n",
    "# Calcul de la différence relative en %\n",
    "diff_pct = ((means_parti - means_reste) / means_reste * 100).round(1)\n",
    "\n",
    "# Créer le DataFrame pour le graphique\n",
    "diff_df = pd.DataFrame(\n",
    "    {\"Variable\": diff_pct.index, \"Différence (%)\": diff_pct.values}\n",
    ").sort_values(\"Différence (%)\", key=abs, ascending=True)\n",
    "\n",
    "fig = px.bar(\n",
    "    diff_df,\n",
    "    x=\"Différence (%)\",\n",
    "    y=\"Variable\",\n",
    "    orientation=\"h\",\n",
    "    title=\"Différence relative des moyennes : Partis vs Restés\",\n",
    "    color=\"Différence (%)\",\n",
    "    color_continuous_scale=[\"#e74c3c\", \"#f5f5f5\", \"#2ecc71\"],\n",
    "    color_continuous_midpoint=0,\n",
    ")\n",
    "\n",
    "fig.add_vline(x=0, line_color=\"black\", line_width=2)\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    xaxis_title=\"Différence relative (%)\",\n",
    "    yaxis_title=\"\",\n",
    "    coloraxis_colorbar_title=\"Diff (%)\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63815306",
   "metadata": {},
   "source": [
    "#### Comment lire ce graphique ?\n",
    "\n",
    "Chaque barre montre **de combien de %** les employés qui partent diffèrent de ceux qui restent **pour cette variable**.\n",
    "\n",
    "| Variable                    | Diff   | Signification concrète                               |\n",
    "| --------------------------- | ------ | ---------------------------------------------------- |\n",
    "| `revenu_mensuel`            | -29.9% | Ceux qui partent gagnent **moins d'argent**          |\n",
    "| `annees_dans_l_entreprise`  | -30.4% | Ceux qui partent ont **moins d'années** d'ancienneté |\n",
    "| `distance_domicile_travail` | +19.3% | Ceux qui partent habitent à **plus de kilomètres**   |\n",
    "\n",
    "**En résumé :**\n",
    "\n",
    "- Barre rouge (négative) : Ceux qui partent ont une valeur **plus basse** pour cette variable\n",
    "- Barre verte (positive) : Ceux qui partent ont une valeur **plus haute** pour cette variable\n",
    "- Proche de 0 : Pas de différence entre les deux groupes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90941077",
   "metadata": {},
   "source": [
    "#### Observations : Variables numériques\n",
    "\n",
    "**Principales différences observées :**\n",
    "\n",
    "| Variable                      | Différence | Observation                          |\n",
    "| ----------------------------- | ---------- | ------------------------------------ |\n",
    "| `nombre_participation_pee`    | -37.6%     | Participation PEE plus faible        |\n",
    "| `annees_dans_le_poste_actuel` | -35.3%     | Ancienneté dans le poste plus faible |\n",
    "| `revenu_mensuel`              | -29.9%     | Salaire plus bas                     |\n",
    "| `distance_domicile_travail`   | +19.3%     | Distance plus grande                 |\n",
    "\n",
    "**Note :** Ce sont des observations descriptives. Le modèle confirmera l'importance réelle de chaque variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943bd016",
   "metadata": {},
   "source": [
    "#### Tableau récapitulatif des statistiques\n",
    "\n",
    "Détail des moyennes et médianes pour chaque groupe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1f65d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau récapitulatif plus robuste\n",
    "stats_parti = df_merged[df_merged[\"statut\"] == \"Parti\"][numeric_cols].agg(\n",
    "    [\"mean\", \"median\"]\n",
    ")\n",
    "stats_reste = df_merged[df_merged[\"statut\"] == \"Resté\"][numeric_cols].agg(\n",
    "    [\"mean\", \"median\"]\n",
    ")\n",
    "\n",
    "comparison_stats = pd.DataFrame(\n",
    "    {\n",
    "        \"Parti_moyenne\": stats_parti.loc[\"mean\"],\n",
    "        \"Parti_médiane\": stats_parti.loc[\"median\"],\n",
    "        \"Resté_moyenne\": stats_reste.loc[\"mean\"],\n",
    "        \"Resté_médiane\": stats_reste.loc[\"median\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Calculer la différence relative\n",
    "comparison_stats[\"Diff_%\"] = (\n",
    "    (comparison_stats[\"Parti_moyenne\"] - comparison_stats[\"Resté_moyenne\"])\n",
    "    / comparison_stats[\"Resté_moyenne\"]\n",
    "    * 100\n",
    ").round(1)\n",
    "\n",
    "# Trier par différence absolue\n",
    "comparison_stats = comparison_stats.sort_values(\"Diff_%\", key=abs, ascending=False)\n",
    "\n",
    "print(\"Statistiques comparatives (triées par |différence|) :\\n\")\n",
    "comparison_stats.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0758a3cf",
   "metadata": {},
   "source": [
    "### 7.2 Analyse des variables catégorielles\n",
    "\n",
    "Visualisons le taux de churn pour chaque modalité de toutes les variables catégorielles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e75d074",
   "metadata": {},
   "source": [
    "#### Taux de churn par modalité\n",
    "\n",
    "Graphique en barres horizontales montrant le taux de départ pour chaque modalité. La ligne rouge = taux moyen global (~16%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67c5d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du taux de churn pour chaque modalité de chaque variable catégorielle\n",
    "churn_data = []\n",
    "\n",
    "for col in categorical_cols:\n",
    "    for modalite in df_merged[col].unique():\n",
    "        subset = df_merged[df_merged[col] == modalite]\n",
    "        taux = (subset[\"a_quitte_l_entreprise\"] == \"Oui\").mean() * 100\n",
    "        effectif = len(subset)\n",
    "        churn_data.append(\n",
    "            {\n",
    "                \"Variable\": col,\n",
    "                \"Modalité\": str(modalite),\n",
    "                \"Taux_churn_%\": round(taux, 1),\n",
    "                \"Effectif\": effectif,\n",
    "            }\n",
    "        )\n",
    "\n",
    "churn_df = pd.DataFrame(churn_data)\n",
    "\n",
    "# Trier par taux de churn décroissant\n",
    "churn_df = churn_df.sort_values(\"Taux_churn_%\", ascending=True)\n",
    "\n",
    "# Créer un label combiné pour l'axe Y\n",
    "churn_df[\"Label\"] = churn_df[\"Variable\"] + \" : \" + churn_df[\"Modalité\"]\n",
    "\n",
    "# Taux global pour référence\n",
    "taux_global = (df_merged[\"a_quitte_l_entreprise\"] == \"Oui\").mean() * 100\n",
    "\n",
    "fig = px.bar(\n",
    "    churn_df,\n",
    "    x=\"Taux_churn_%\",\n",
    "    y=\"Label\",\n",
    "    orientation=\"h\",\n",
    "    title=\"Taux de churn par modalité (toutes variables catégorielles)\",\n",
    "    color=\"Taux_churn_%\",\n",
    "    color_continuous_scale=[\"#2ecc71\", \"#f39c12\", \"#e74c3c\"],\n",
    "    hover_data=[\"Variable\", \"Modalité\", \"Effectif\"],\n",
    "    labels={\"Taux_churn_%\": \"Taux de churn (%)\", \"Label\": \"\"},\n",
    ")\n",
    "\n",
    "# Ligne de référence (taux global)\n",
    "fig.add_vline(\n",
    "    x=taux_global,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"red\",\n",
    "    line_width=2,\n",
    "    annotation_text=f\"Moyenne: {taux_global:.1f}%\",\n",
    "    annotation_position=\"top\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=max(600, len(churn_df) * 25),\n",
    "    xaxis_title=\"Taux de churn (%)\",\n",
    "    yaxis_title=\"\",\n",
    "    showlegend=False,\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\n",
    "    \"\\nLecture : Les barres a DROITE de la ligne rouge ont un taux de churn SUPERIEUR a la moyenne.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1e7fed",
   "metadata": {},
   "source": [
    "#### Observations : Variables catégorielles\n",
    "\n",
    "**Taux de churn élevé :** Représentant Commercial (39.8%), heures sup Oui (30.5%), Célibataire (25.5%)\n",
    "\n",
    "**Taux de churn faible :** Directeur Technique (2.5%), Manager (6.9%), heures sup Non (10.4%)\n",
    "\n",
    "**Facteurs à explorer :** heures sup, poste, statut marital, déplacements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e78f13e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 7.3 Observations préliminaires\n",
    "\n",
    "**Variables à explorer dans le modèle :**\n",
    "\n",
    "- Numériques : `nombre_participation_pee`, `annees_dans_le_poste_actuel`, `revenu_mensuel`\n",
    "- Catégorielles : `heure_supplementaires`, `poste`, `statut_marital`\n",
    "\n",
    "**Attention :**\n",
    "\n",
    "- Déséquilibre 84%/16% : stratification + gestion du déséquilibre nécessaire\n",
    "- Ces observations sont **descriptives** : le modèle validera quelles variables sont réellement prédictives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cf5ceb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Partie 2 : Feature Engineering\n",
    "\n",
    "Dans cette partie, nous allons :\n",
    "\n",
    "1. **Nettoyer les donnees** : doublons, outliers, colonnes inutiles\n",
    "2. **Analyser les correlations** : matrice de Pearson, suppression des variables trop correlees\n",
    "3. **Encoder les variables categorielles** : OneHotEncoder pour les modeles\n",
    "4. **Creer X et y** : preparation finale pour la modelisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e376d06",
   "metadata": {},
   "source": [
    "## 8. Nettoyage des donnees\n",
    "\n",
    "### 8.1 Verification des doublons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ead847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"  Nombre de lignes dupliquees : {df_merged.duplicated().sum()}\")\n",
    "print(f\"\\nDoublons sur 'id_employee' : {df_merged['id_employee'].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57167662",
   "metadata": {},
   "source": [
    "### 8.2 Detection des outliers (methode IQR)\n",
    "\n",
    "**Qu'est-ce que la methode IQR (Interquartile Range) ?**\n",
    "\n",
    "L'IQR est une methode statistique robuste pour detecter les valeurs aberrantes :\n",
    "\n",
    "**Calcul des bornes :**\n",
    "\n",
    "- Borne inferieure = Q1 - 1.5 x IQR\n",
    "- Borne superieure = Q3 + 1.5 x IQR\n",
    "\n",
    "Toute valeur en dehors de ces bornes est consideree comme un **outlier**.\n",
    "\n",
    "**Pourquoi IQR plutot que Z-score ?**\n",
    "\n",
    "- IQR est base sur les **quartiles** (pas la moyenne)\n",
    "- Donc **insensible aux valeurs extremes** elles-memes\n",
    "- Plus adapte aux distributions non-normales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf2339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection des outliers avec la methode IQR (Interquartile Range)\n",
    "def detect_outliers_iqr(df, columns):\n",
    "    \"\"\"Detecte les outliers pour chaque colonne numerique avec la methode IQR.\"\"\"\n",
    "    outliers_summary = []\n",
    "\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        n_outliers = len(outliers)\n",
    "        pct_outliers = (n_outliers / len(df)) * 100\n",
    "\n",
    "        if n_outliers > 0:\n",
    "            outliers_summary.append(\n",
    "                {\n",
    "                    \"Variable\": col,\n",
    "                    \"Nb outliers\": n_outliers,\n",
    "                    \"% outliers\": round(pct_outliers, 1),\n",
    "                    \"Borne inf\": round(lower_bound, 2),\n",
    "                    \"Borne sup\": round(upper_bound, 2),\n",
    "                    \"Min reel\": round(df[col].min(), 2),\n",
    "                    \"Max reel\": round(df[col].max(), 2),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(outliers_summary)\n",
    "\n",
    "\n",
    "# Exclure les colonnes ID et la cible pour l'analyse des outliers\n",
    "cols_to_check = [\n",
    "    col\n",
    "    for col in numeric_cols\n",
    "    if col not in [\"id_employee\", \"eval_number\", \"code_sondage\"]\n",
    "]\n",
    "outliers_df = detect_outliers_iqr(df_merged, cols_to_check)\n",
    "\n",
    "print(f\"Variables avec outliers : {len(outliers_df)} / {len(cols_to_check)}\")\n",
    "print()\n",
    "if len(outliers_df) > 0:\n",
    "    display(outliers_df.sort_values(\"% outliers\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbe8f28",
   "metadata": {},
   "source": [
    "**Decision sur les outliers :**\n",
    "\n",
    "Les outliers detectes sont des valeurs coherentes dans un contexte RH :\n",
    "\n",
    "- **Revenus eleves** : salaires de cadres superieurs (jusqu'a 19 999 EUR)\n",
    "- **Anciennete elevee** : employes fideles (jusqu'a 40 ans)\n",
    "- **Formations** : 6 formations maximum, valeur plausible\n",
    "\n",
    "Ces valeurs ne sont pas des erreurs de saisie mais des cas legitimes. Nous les **conservons** car :\n",
    "\n",
    "1. Les modeles tree-based (Random Forest, XGBoost) gerent bien les outliers\n",
    "2. Ces profils extremes peuvent etre pertinents pour predire le churn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef98ecb",
   "metadata": {},
   "source": [
    "### 8.3 Identification des colonnes a supprimer\n",
    "\n",
    "**Pourquoi supprimer certaines colonnes ?**\n",
    "\n",
    "1. **Colonnes ID** (id_employee, eval_number, code_sondage)\n",
    "   - Ce sont des identifiants uniques (1, 2, 3...)\n",
    "   - Aucune valeur predictive : le modele ne peut pas apprendre que \"employe 42\" part plus souvent\n",
    "\n",
    "2. **Colonnes a variance nulle**\n",
    "   - Une colonne avec la **meme valeur pour tous** (ex: `nombre_heures_travailless = 80` pour tout le monde)\n",
    "   - Aucune information discriminante : impossible de differencier partis vs restes\n",
    "\n",
    "3. **Colonnes redondantes**\n",
    "   - `a_quitte_l_entreprise` et `statut` contiennent la meme information que notre cible\n",
    "   - Les garder = **data leakage** (le modele \"triche\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33bed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Colonnes actuelles du dataset :\")\n",
    "print(df_merged.columns.tolist())\n",
    "\n",
    "id_columns = [\"id_employee\", \"eval_number\", \"code_sondage\"]\n",
    "\n",
    "target_column = \"depart\"\n",
    "\n",
    "# Colonnes a variance nulle ou quasi-nulle\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"\\nVerification des colonnes a faible variance :\")\n",
    "for col in df_merged.columns:\n",
    "    unique_ratio = df_merged[col].nunique() / len(df_merged)\n",
    "    if df_merged[col].nunique() <= 2 and col != target_column:\n",
    "        print(\n",
    "            f\"  {col} : {df_merged[col].nunique()} valeurs uniques -> {df_merged[col].value_counts().to_dict()}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f78867",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = {\n",
    "    # Colonnes ID (pas de valeur predictive)\n",
    "    \"id_employee\": \"Identifiant unique - pas de valeur predictive\",\n",
    "    \"eval_number\": \"Identifiant evaluation - pas de valeur predictive\",\n",
    "    \"code_sondage\": \"Identifiant sondage - pas de valeur predictive\",\n",
    "    # Colonnes a variance nulle (meme valeur pour tous)\n",
    "    \"nombre_heures_travailless\": \"Variance nulle - toujours 80\",\n",
    "    \"nombre_employee_sous_responsabilite\": \"Variance nulle - toujours 1\",\n",
    "    \"ayant_enfants\": \"Variance nulle - toujours Y\",\n",
    "    # Colonne redondante avec la cible\n",
    "    \"a_quitte_l_entreprise\": 'Redondante avec \"statut\" (variable cible)',\n",
    "    \"statut\": 'Redondante - nous utiliserons \"depart\" comme cible binaire',\n",
    "}\n",
    "\n",
    "print(\"Colonnes a supprimer :\")\n",
    "for col, reason in columns_to_drop.items():\n",
    "    print(f\"  - {col}: {reason}\")\n",
    "\n",
    "print(f\"\\nTotal : {len(columns_to_drop)} colonnes a supprimer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21c2f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation du DataFrame nettoye\n",
    "df_clean = df_merged.copy()\n",
    "\n",
    "# Creation de la variable cible binaire 'depart' (0 = Reste, 1 = Churn)\n",
    "df_clean[\"depart\"] = (df_clean[\"statut\"] == \"Parti\").astype(int)\n",
    "\n",
    "# Suppression des colonnes identifiees\n",
    "df_clean = df_clean.drop(columns=list(columns_to_drop.keys()))\n",
    "\n",
    "print(f\"Dataset initial : {df_merged.shape[0]} lignes x {df_merged.shape[1]} colonnes\")\n",
    "print(f\"Dataset nettoye : {df_clean.shape[0]} lignes x {df_clean.shape[1]} colonnes\")\n",
    "print(f\"\\nColonnes restantes ({df_clean.shape[1]}) :\")\n",
    "print(df_clean.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb6926f",
   "metadata": {},
   "source": [
    "## 9. Analyse des correlations\n",
    "\n",
    "**Pourquoi analyser les correlations ?**\n",
    "\n",
    "1. **Identifier les relations lineaires** entre variables\n",
    "2. **Detecter la multicolinearite** : si 2 variables sont tres correlees (|r| > 0.7), elles apportent la meme information → on peut en supprimer une\n",
    "3. **Comprendre les liens avec la cible** : quelles variables sont les plus correlees avec le depart ?\n",
    "\n",
    "### 9.1 Matrice de correlation de Pearson\n",
    "\n",
    "**Pearson** mesure les correlations **lineaires** :\n",
    "\n",
    "- r = +1 : correlation positive parfaite\n",
    "- r = 0 : pas de correlation lineaire\n",
    "- r = -1 : correlation negative parfaite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbea7b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Selection des colonnes numeriques pour la correlation\n",
    "numeric_cols_clean = df_clean.select_dtypes(\n",
    "    include=[\"int64\", \"float64\"]\n",
    ").columns.tolist()\n",
    "print(f\"Variables numeriques pour la correlation : {len(numeric_cols_clean)}\")\n",
    "\n",
    "# Calcul de la matrice de correlation\n",
    "corr_matrix = df_clean[numeric_cols_clean].corr()\n",
    "\n",
    "fig_corr = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=corr_matrix.values,\n",
    "        x=corr_matrix.columns,\n",
    "        y=corr_matrix.columns,\n",
    "        colorscale=\"RdBu_r\",\n",
    "        zmid=0,\n",
    "        text=corr_matrix.values.round(2),\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 8},\n",
    "        colorbar=dict(title=\"Correlation\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_corr.update_layout(\n",
    "    title=\"Matrice de Correlation de Pearson (toutes variables numeriques)\",\n",
    "    width=1000,\n",
    "    height=900,\n",
    "    xaxis=dict(tickangle=-45, tickfont=dict(size=9)),\n",
    "    yaxis=dict(tickfont=dict(size=9)),\n",
    ")\n",
    "\n",
    "fig_corr.show()\n",
    "\n",
    "# Correlations avec la cible\n",
    "print(\"\\nCORRELATIONS AVEC LA CIBLE 'depart' :\")\n",
    "print(\"-\" * 50)\n",
    "target_corr = corr_matrix[\"depart\"].drop(\"depart\").sort_values(key=abs, ascending=False)\n",
    "for var, corr_value in target_corr.items():\n",
    "    print(f\"   {var:40} : {corr_value:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0f9a02",
   "metadata": {},
   "source": [
    "### 9.1b Matrice de correlation de Spearman\n",
    "\n",
    "**Spearman** mesure les correlations **monotones** (pas forcement lineaires) :\n",
    "\n",
    "- Plus robuste aux outliers\n",
    "- Detecte les relations non-lineaires croissantes/decroissantes\n",
    "- Recommande en complement de Pearson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3fb5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliser les colonnes numeriques actuelles de df_clean\n",
    "num_cols_current = df_clean.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "corr_spearman = df_clean[num_cols_current].corr(method=\"spearman\")\n",
    "\n",
    "# Visualisation\n",
    "fig_spearman = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=corr_spearman.values,\n",
    "        x=corr_spearman.columns,\n",
    "        y=corr_spearman.columns,\n",
    "        colorscale=\"RdBu_r\",\n",
    "        zmid=0,\n",
    "        text=corr_spearman.values.round(2),\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 8},\n",
    "        colorbar=dict(title=\"Correlation\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_spearman.update_layout(\n",
    "    title=\"Matrice de Correlation de Spearman (correlations monotones)\",\n",
    "    width=1000,\n",
    "    height=900,\n",
    "    xaxis=dict(tickangle=-45, tickfont=dict(size=9)),\n",
    "    yaxis=dict(tickfont=dict(size=9)),\n",
    ")\n",
    "\n",
    "fig_spearman.show()\n",
    "\n",
    "# Comparaison Pearson vs Spearman pour la cible\n",
    "# La variable cible est dans corr_matrix (de Pearson) sous le nom \"depart\"\n",
    "target_name = \"depart\" if \"depart\" in corr_spearman.columns else \"depart_volontaire\"\n",
    "\n",
    "if target_name in corr_spearman.columns:\n",
    "    print(f\"\\nCOMPARAISON PEARSON vs SPEARMAN (correlations avec '{target_name}') :\")\n",
    "    print(\"-\" * 65)\n",
    "    spearman_corr = (\n",
    "        corr_spearman[target_name]\n",
    "        .drop(target_name)\n",
    "        .sort_values(key=abs, ascending=False)\n",
    "    )\n",
    "    print(f\"{'Variable':<40} {'Pearson':>12} {'Spearman':>12}\")\n",
    "    print(\"-\" * 65)\n",
    "    for var in spearman_corr.index[:10]:\n",
    "        if var in target_corr.index:\n",
    "            p = target_corr[var]\n",
    "            s = spearman_corr[var]\n",
    "            print(f\"{var:<40} {p:>+12.3f} {s:>+12.3f}\")\n",
    "else:\n",
    "    print(\"Note: La variable cible n'est plus dans df_clean (separee dans 'y').\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c11216",
   "metadata": {},
   "source": [
    "### 9.2 Identification des correlations fortes (|r| > 0.7)\n",
    "\n",
    "**Pourquoi c'est un probleme ?**\n",
    "\n",
    "Si deux variables sont **tres correlees** (ex: `revenu_mensuel` et `niveau_hierarchique_poste` a 0.95), elles apportent **la meme information** au modele.\n",
    "\n",
    "**Consequences de la multicolinearite :**\n",
    "\n",
    "- Modeles **lineaires instables** (coefficients aberrants)\n",
    "- **Redondance** d'information\n",
    "- Difficulte d'interpretation\n",
    "\n",
    "**Solution :** Supprimer une des deux variables. On garde celle qui est :\n",
    "\n",
    "- La plus **correlee avec la cible**, ou\n",
    "- La plus **interpretable metier**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e32385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification des paires de variables fortement correlees (|r| > 0.7)\n",
    "threshold = 0.7\n",
    "high_corr_pairs = []\n",
    "\n",
    "# Parcourir le triangle inferieur de la matrice (sans la diagonale)\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "            high_corr_pairs.append(\n",
    "                {\n",
    "                    \"Variable 1\": corr_matrix.columns[j],\n",
    "                    \"Variable 2\": corr_matrix.columns[i],\n",
    "                    \"Correlation\": round(corr_matrix.iloc[i, j], 3),\n",
    "                }\n",
    "            )\n",
    "\n",
    "high_corr_df = pd.DataFrame(high_corr_pairs).sort_values(\"Correlation\", ascending=False)\n",
    "print(f\"Paires de variables avec correlation |r| > {threshold} :\")\n",
    "print()\n",
    "display(high_corr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f2ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision : quelles variables supprimer pour eviter la multicolinearite ?\n",
    "# Critere : garder la variable la plus interpretable ou la plus correlee avec la cible\n",
    "\n",
    "# Correlation de chaque variable avec la cible 'depart'\n",
    "print(\"Correlation avec la cible 'depart' :\")\n",
    "target_corr = corr_matrix[\"depart\"].drop(\"depart\").abs().sort_values(ascending=False)\n",
    "print(target_corr.head(10))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"\\nAnalyse des paires correlees :\")\n",
    "for _, row in high_corr_df.iterrows():\n",
    "    var1, var2 = row[\"Variable 1\"], row[\"Variable 2\"]\n",
    "    corr1 = abs(corr_matrix.loc[\"depart\", var1])\n",
    "    corr2 = abs(corr_matrix.loc[\"depart\", var2])\n",
    "    print(\n",
    "        f\"\\n{var1} (|r| avec depart = {corr1:.3f}) vs {var2} (|r| avec depart = {corr2:.3f})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650d936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des variables redondantes\n",
    "# Logique : garder les variables les plus correlees avec la cible\n",
    "\n",
    "cols_to_remove_corr = [\n",
    "    \"niveau_hierarchique_poste\",  # Tres correle avec revenu_mensuel (0.95), moins interpretable\n",
    "    \"annees_dans_l_entreprise\",  # Correle avec annees_dans_le_poste_actuel et annes_sous_responsable\n",
    "]\n",
    "\n",
    "print(\"Variables supprimees pour multicolinearite :\")\n",
    "for col in cols_to_remove_corr:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Application de la suppression\n",
    "df_clean = df_clean.drop(columns=cols_to_remove_corr)\n",
    "print(\n",
    "    f\"\\nDataset apres suppression : {df_clean.shape[0]} lignes x {df_clean.shape[1]} colonnes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d976f594",
   "metadata": {},
   "source": [
    "### 9.3 Justification du choix des variables supprimées\n",
    "\n",
    "**Pourquoi supprimer `niveau_hierarchique_poste` et `annees_dans_l_entreprise` ?**\n",
    "\n",
    "Ces deux variables semblent intuitivement importantes pour prédire le départ. Cependant, leur suppression est justifiée par le **problème de multicolinéarité** :\n",
    "\n",
    "**Corrélations détectées (|r| > 0.7) :**\n",
    "\n",
    "| Variable 1                 | Variable 2                      | Corrélation |\n",
    "| -------------------------- | ------------------------------- | ----------- |\n",
    "| `revenu_mensuel`           | `niveau_hierarchique_poste`     | **0.95**    |\n",
    "| `annee_experience_totale`  | `niveau_hierarchique_poste`     | 0.78        |\n",
    "| `annees_dans_l_entreprise` | `annees_dans_le_poste_actuel`   | 0.76        |\n",
    "| `annees_dans_l_entreprise` | `annes_sous_responsable_actuel` | 0.77        |\n",
    "\n",
    "**Le problème** : Quand deux variables sont corrélées à 95%, elles apportent **presque la même information**. Le modèle ne sait pas laquelle utiliser → coefficients instables et interprétation SHAP biaisée.\n",
    "\n",
    "**Corrélations avec la cible `depart` :**\n",
    "\n",
    "| Variable                        | Corrélation avec depart    |\n",
    "| ------------------------------- | -------------------------- |\n",
    "| `niveau_hierarchique_poste`     | 0.169                      |\n",
    "| `annees_dans_le_poste_actuel`   | 0.161                      |\n",
    "| `revenu_mensuel`                | 0.160                      |\n",
    "| `annes_sous_responsable_actuel` | 0.156                      |\n",
    "| `annees_dans_l_entreprise`      | **0.134** (la plus faible) |\n",
    "\n",
    "**Critères de sélection (logique métier RH) :**\n",
    "\n",
    "1. **`revenu_mensuel` gardé vs `niveau_hierarchique_poste` supprimé** :\n",
    "   - Le salaire est **plus concret et actionnable** pour les RH qu'un \"niveau 3 vs niveau 4\"\n",
    "   - Corrélations quasi égales avec la cible (0.160 vs 0.169)\n",
    "   - Avec r=0.95, l'information de `niveau_hierarchique_poste` est **déjà contenue** dans `revenu_mensuel`\n",
    "\n",
    "2. **`annees_dans_le_poste_actuel` gardé vs `annees_dans_l_entreprise` supprimé** :\n",
    "   - `annees_dans_le_poste_actuel` a une corrélation **plus forte** avec le départ (0.161 vs 0.134)\n",
    "   - Plus pertinent métier : c'est la **stagnation dans le poste** qui pousse au départ, pas l'ancienneté globale\n",
    "\n",
    "**Conclusion** : On ne perd pas d'information prédictive, car les variables conservées (`revenu_mensuel`, `annees_dans_le_poste_actuel`) captent l'essentiel de l'information des variables supprimées, tout en étant plus interprétables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e769835",
   "metadata": {},
   "source": [
    "### 9.4 Conversion de `augementation_salaire_precedente` en numérique\n",
    "\n",
    "**Problème identifié :** La colonne `augementation_salaire_precedente` contient des valeurs comme \"11 %\", \"23 %\" qui sont stockées comme **texte (object)**.\n",
    "\n",
    "**Pourquoi convertir en numérique ?**\n",
    "\n",
    "| Approche                      | Nb features          | Compréhension modèle             | Risque overfitting |\n",
    "| ----------------------------- | -------------------- | -------------------------------- | ------------------ |\n",
    "| **Catégorielle (défaut)**     | 14 colonnes (OneHot) | ❌ Perd l'ordre et les distances | ⚠️ Plus élevé      |\n",
    "| **Numérique (best practice)** | 1 colonne            | ✅ 11% < 12% < 23%               | ✅ Réduit          |\n",
    "\n",
    "**Avantages de la conversion :**\n",
    "\n",
    "- Le modèle comprend que **23% > 11%** (relation ordinale préservée)\n",
    "- Le modèle comprend que **23% - 11% = 12 points** (distances préservées)\n",
    "- **1 feature** au lieu de **14** → moins de dimensions → moins d'overfitting\n",
    "- Coefficient plus **interprétable** : \"+1% d'augmentation = X% de risque de départ\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eda9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion de \"11 %\" -> 11.0 (valeur numerique)\n",
    "# On garde la valeur en pourcentage (11, 12, 23...) plutot qu'en decimal (0.11, 0.12...)\n",
    "\n",
    "print(\"Avant conversion :\")\n",
    "print(f\"  Type: {df_clean['augementation_salaire_precedente'].dtype}\")\n",
    "print(\n",
    "    f\"  Valeurs uniques: {df_clean['augementation_salaire_precedente'].unique()[:5]}...\"\n",
    ")\n",
    "\n",
    "# Suppression du \" %\" et conversion en float\n",
    "df_clean[\"augementation_salaire_precedente\"] = (\n",
    "    df_clean[\"augementation_salaire_precedente\"]\n",
    "    .str.replace(\" %\", \"\", regex=False)\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "print(\"\\nApres conversion :\")\n",
    "print(f\"  Type: {df_clean['augementation_salaire_precedente'].dtype}\")\n",
    "print(\n",
    "    f\"  Valeurs uniques: {sorted(df_clean['augementation_salaire_precedente'].unique())}\"\n",
    ")\n",
    "print(f\"  Min: {df_clean['augementation_salaire_precedente'].min()}%\")\n",
    "print(f\"  Max: {df_clean['augementation_salaire_precedente'].max()}%\")\n",
    "print(f\"  Moyenne: {df_clean['augementation_salaire_precedente'].mean():.1f}%\")\n",
    "\n",
    "print(\"\\n✅ La colonne sera maintenant traitee comme NUMERIQUE dans le Pipeline\")\n",
    "print(\"   → StandardScaler au lieu de OneHotEncoder\")\n",
    "print(\"   → 1 feature au lieu de 14\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e791b",
   "metadata": {},
   "source": [
    "## 10. Feature Engineering - Creation de nouvelles variables\n",
    "\n",
    "**Pourquoi creer de nouvelles features ?**\n",
    "\n",
    "\"features supplementaires par rapport aux donnees d'origine\"\n",
    "\n",
    "**Objectif :** Creer des variables qui capturent des **informations metier** que les colonnes brutes ne montrent pas directement.\n",
    "\n",
    "**Features creees (3) :**\n",
    "\n",
    "| Feature                    | Formule                                        | Interpretation metier                            |\n",
    "| -------------------------- | ---------------------------------------------- | ------------------------------------------------ |\n",
    "| `ratio_salaire_experience` | revenu_mensuel / (experience + 1)              | Employe sous-paye par rapport a son experience ? |\n",
    "| `stagnation_poste`         | annees_dans_le_poste - annees_depuis_promotion | Employe bloque sans evolution ?                  |\n",
    "| `satisfaction_globale`     | moyenne des 4 satisfactions employee           | Score synthetique de bien-etre                   |\n",
    "\n",
    "**Pourquoi seulement 3 ?**\n",
    "\n",
    "- Eviter l'**overfitting** (trop de features pour peu de donnees)\n",
    "- Chaque feature doit avoir un **sens metier RH**\n",
    "- Le dataset est deja riche (50+ colonnes apres encodage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9934a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 1 : Ratio salaire / experience\n",
    "# Interpretation : Un ratio bas = potentiellement sous-paye\n",
    "# Note: +1 pour eviter division par zero si experience = 0\n",
    "df_clean[\"ratio_salaire_experience\"] = df_clean[\"revenu_mensuel\"] / (\n",
    "    df_clean[\"annee_experience_totale\"] + 1\n",
    ")\n",
    "\n",
    "# Feature 2 : Stagnation de carriere\n",
    "# Interpretation : Valeur elevee = beaucoup d'annees dans le poste sans promotion recente\n",
    "df_clean[\"stagnation_poste\"] = (\n",
    "    df_clean[\"annees_dans_le_poste_actuel\"]\n",
    "    - df_clean[\"annees_depuis_la_derniere_promotion\"]\n",
    ")\n",
    "\n",
    "# Feature 3 : Satisfaction globale (moyenne des 4 satisfactions)\n",
    "# Interpretation : Score synthetique qui resume le bien-etre au travail\n",
    "cols_satisfaction = [\n",
    "    \"satisfaction_employee_environnement\",\n",
    "    \"satisfaction_employee_nature_travail\",\n",
    "    \"satisfaction_employee_equipe\",\n",
    "    \"satisfaction_employee_equilibre_pro_perso\",\n",
    "]\n",
    "df_clean[\"satisfaction_globale\"] = df_clean[cols_satisfaction].mean(axis=1)\n",
    "\n",
    "print(\"3 features creees avec succes !\")\n",
    "print(f\"\\nNouvelle shape du dataframe : {df_clean.shape}\")\n",
    "print(\"\\nApercu des nouvelles features :\")\n",
    "df_clean[\n",
    "    [\"ratio_salaire_experience\", \"stagnation_poste\", \"satisfaction_globale\"]\n",
    "].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0221235",
   "metadata": {},
   "source": [
    "### 10.2 Verification de la pertinence des features\n",
    "\n",
    "Verifions la correlation de nos nouvelles features avec la variable cible `depart` :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724aa73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation des nouvelles features avec depart (variable cible)\n",
    "new_features = [\"ratio_salaire_experience\", \"stagnation_poste\", \"satisfaction_globale\"]\n",
    "\n",
    "# Creer un dataframe temporaire avec les nouvelles features et la cible\n",
    "temp_df = df_clean[new_features].copy()\n",
    "temp_df[\"depart\"] = df_clean[\n",
    "    \"depart\"\n",
    "].values  # La cible est encore dans df_clean a ce stade\n",
    "\n",
    "correlations = temp_df.corr()[\"depart\"].drop(\"depart\")\n",
    "\n",
    "print(\"Correlation avec depart (variable cible) :\")\n",
    "for feat, corr in correlations.items():\n",
    "    signe = \"🔴\" if corr > 0 else \"🟢\"\n",
    "    interpretation = \"↑ quitte plus\" if corr > 0 else \"↓ reste plus\"\n",
    "    print(f\"{signe} {feat}: {corr:.4f} ({interpretation})\")\n",
    "\n",
    "print(\"\\nInterpretation :\")\n",
    "print(\"   - satisfaction_globale : plus les gens sont satisfaits, moins ils partent\")\n",
    "print(\n",
    "    \"   - stagnation_poste : correlation negative = ceux qui stagnent RESTENT (profils seniors stables)\"\n",
    ")\n",
    "print(\n",
    "    \"   - ratio_salaire_experience : les mieux payes par rapport a leur experience partent plus\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8927c9",
   "metadata": {},
   "source": [
    "## 11. Pipeline de preprocessing avec ColumnTransformer\n",
    "\n",
    "### Pourquoi utiliser un Pipeline ?\n",
    "\n",
    "| Avantage                     | Explication                                                               |\n",
    "| ---------------------------- | ------------------------------------------------------------------------- |\n",
    "| ✅ **Evite le data leakage** | Le preprocessing est applique UNIQUEMENT sur le train a chaque fold de CV |\n",
    "| ✅ **Code propre**           | Tout le preprocessing est encapsule dans un seul objet                    |\n",
    "| ✅ **Reproductible**         | Facile a reutiliser et deployer                                           |\n",
    "| ✅ **Compatible CV**         | S'integre parfaitement avec `cross_val_score` et `GridSearchCV`           |\n",
    "\n",
    "### 11.1 Identification des colonnes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187f5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separation features / cible AVANT le pipeline\n",
    "y = df_clean[\"depart\"]\n",
    "X = df_clean.drop(columns=[\"depart\"])\n",
    "\n",
    "# Identification automatique des colonnes numeriques et categorielles\n",
    "num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "print(\"Colonnes identifiees pour le Pipeline :\")\n",
    "print(\n",
    "    f\"\\n  Numeriques ({len(num_cols)}) : {num_cols[:5]}{'...' if len(num_cols) > 5 else ''}\"\n",
    ")\n",
    "print(f\"\\n  Categorielles ({len(cat_cols)}) :\")\n",
    "for col in cat_cols:\n",
    "    unique_vals = X[col].unique()\n",
    "    print(f\"    - {col} ({len(unique_vals)} modalites)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f44c983",
   "metadata": {},
   "source": [
    "### 11.2 Creation du ColumnTransformer\n",
    "\n",
    "**ColumnTransformer** applique des transformations differentes selon le type de colonne :\n",
    "\n",
    "| Type de colonne  | Transformation                | Parametre                  |\n",
    "| ---------------- | ----------------------------- | -------------------------- |\n",
    "| **Numerique**    | `StandardScaler()`            | Moyenne=0, Ecart-type=1    |\n",
    "| **Categorielle** | `OneHotEncoder(drop='first')` | Evite colinearite parfaite |\n",
    "\n",
    "**Parametre `remainder='passthrough'`** : conserve les colonnes non transformees (si elles existent).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a743e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Creation du ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_cols),  # Standardisation des numeriques\n",
    "        (\n",
    "            \"cat\",\n",
    "            OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\", sparse_output=False),\n",
    "            cat_cols,\n",
    "        ),  # Encodage des categorielles\n",
    "    ],\n",
    "    remainder=\"passthrough\",  # Conserver les autres colonnes si elles existent\n",
    ")\n",
    "\n",
    "print(\"\\nTransformations definies :\")\n",
    "print(f\"  - 'num' : StandardScaler sur {len(num_cols)} colonnes numeriques\")\n",
    "print(f\"  - 'cat' : OneHotEncoder sur {len(cat_cols)} colonnes categorielles\")\n",
    "print(\"\\nLe preprocessor sera FIT sur X_train uniquement (pas de data leakage)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7739e3d5",
   "metadata": {},
   "source": [
    "### 11.3 Split Train/Test avec stratification\n",
    "\n",
    "**❓ Question légitime : Pourquoi faire un split si on fait de la cross-validation ?**\n",
    "\n",
    "On fait les **DEUX** pour des raisons différentes :\n",
    "\n",
    "| Étape                                       | Objectif                                              | Utilisation                          |\n",
    "| ------------------------------------------- | ----------------------------------------------------- | ------------------------------------ |\n",
    "| **Split Train/Test (80/20)**                | Avoir un **jeu de test FINAL jamais touché**          | Évaluation finale du meilleur modèle |\n",
    "| **Cross-validation (sur Train uniquement)** | **Comparer les modèles** et tuner les hyperparamètres | Sélection du meilleur modèle         |\n",
    "\n",
    "**Pourquoi ?** Si on fait la CV sur **tout le dataset**, on n'a plus de données \"fraîches\" pour vérifier si le modèle généralise vraiment. Le test est le **juge final impartial**.\n",
    "\n",
    "**Workflow preprocessing (éviter data leakage) :**\n",
    "\n",
    "| Etape | Action                                   | Explication                                    |\n",
    "| ----- | ---------------------------------------- | ---------------------------------------------- |\n",
    "| 1     | `X, y = separation features/cible`       | Séparer les variables explicatives de la cible |\n",
    "| 2     | `X_train, X_test = split(X, y)`          | Split **avant** preprocessing                  |\n",
    "| 3     | `preprocessor.fit(X_train)`              | Fit sur train **UNIQUEMENT**                   |\n",
    "| 4     | `X_train_processed = transform(X_train)` | Transform train                                |\n",
    "| 5     | `X_test_processed = transform(X_test)`   | Transform test (mêmes paramètres du train)     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5695bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split stratifie AVANT le fit du preprocessor\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y,  # Important pour le desequilibre de classes\n",
    ")\n",
    "\n",
    "print(\"Split train/test avec stratification :\")\n",
    "print(f\"\\n  Train : {X_train.shape[0]} lignes ({X_train.shape[0] / len(X) * 100:.0f}%)\")\n",
    "print(f\"    - Churn : {y_train.sum()} ({y_train.mean() * 100:.1f}%)\")\n",
    "print(\n",
    "    f\"    - Non-churn : {len(y_train) - y_train.sum()} ({(1 - y_train.mean()) * 100:.1f}%)\"\n",
    ")\n",
    "\n",
    "print(f\"\\n  Test : {X_test.shape[0]} lignes ({X_test.shape[0] / len(X) * 100:.0f}%)\")\n",
    "print(f\"    - Churn : {y_test.sum()} ({y_test.mean() * 100:.1f}%)\")\n",
    "print(\n",
    "    f\"    - Non-churn : {len(y_test) - y_test.sum()} ({(1 - y_test.mean()) * 100:.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7f810e",
   "metadata": {},
   "source": [
    "### 11.4 Application du preprocessor (fit_transform sur train, transform sur test)\n",
    "\n",
    "**C'est ici que la magie du Pipeline opere :**\n",
    "\n",
    "- `fit_transform(X_train)` : calcule les parametres (moyenne, ecart-type, modalites) ET transforme\n",
    "- `transform(X_test)` : utilise les parametres du train pour transformer le test\n",
    "\n",
    "**Avantage majeur** : Quand on utilisera `cross_val_score`, le fit sera automatiquement refait sur chaque fold !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1353beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit sur train, transform sur train et test\n",
    "X_train_processed = preprocessor.fit_transform(X_train)  # FIT + TRANSFORM\n",
    "X_test_processed = preprocessor.transform(X_test)  # TRANSFORM seulement\n",
    "\n",
    "# Recuperation des noms de colonnes pour lisibilite\n",
    "# (Les colonnes numeriques gardent leur nom, les categorielles sont encodees)\n",
    "num_feature_names = num_cols\n",
    "cat_feature_names = (\n",
    "    preprocessor.named_transformers_[\"cat\"].get_feature_names_out(cat_cols).tolist()\n",
    ")\n",
    "all_feature_names = num_feature_names + cat_feature_names\n",
    "\n",
    "print(\"Preprocessing applique avec succes !\")\n",
    "print(\"\\nDimensions apres transformation :\")\n",
    "print(f\"  X_train_processed : {X_train_processed.shape}\")\n",
    "print(f\"  X_test_processed  : {X_test_processed.shape}\")\n",
    "\n",
    "print(f\"\\nNombre de features finales : {len(all_feature_names)}\")\n",
    "print(f\"  - Numeriques (standardisees) : {len(num_feature_names)}\")\n",
    "print(f\"  - Categorielles (encodees)   : {len(cat_feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b653394",
   "metadata": {},
   "source": [
    "### 11.5 Verification : pas de data leakage\n",
    "\n",
    "Verifions que le StandardScaler a bien ete fit sur le train uniquement :\n",
    "\n",
    "- **Train** : moyenne ≈ 0, ecart-type ≈ 1\n",
    "- **Test** : moyenne ≠ 0 exactement (normal, car les parametres viennent du train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e07bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_indices = list(range(len(num_cols)))\n",
    "\n",
    "print(\"Verification du StandardScaler (colonnes numeriques) :\")\n",
    "print(f\"\\n  Train - Moyenne  : {X_train_processed[:, num_indices].mean():.6f}\")\n",
    "print(f\"  Train - Std      : {X_train_processed[:, num_indices].std():.6f}\")\n",
    "print(f\"\\n  Test  - Moyenne  : {X_test_processed[:, num_indices].mean():.6f}\")\n",
    "print(f\"  Test  - Std      : {X_test_processed[:, num_indices].std():.6f}\")\n",
    "\n",
    "print(\"\\nPas de data leakage : le test n'a pas exactement moyenne=0\")\n",
    "print(\"   (les parametres du scaler viennent du train)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904c643c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Synthèse Partie 2 : Feature Engineering\n",
    "\n",
    "### Nettoyage effectué\n",
    "\n",
    "- ✅ Aucun doublon détecté\n",
    "- ✅ Outliers conservés (valeurs RH légitimes : hauts salaires, ancienneté élevée)\n",
    "- ✅ 8 colonnes supprimées : identifiants, variance nulle, redondantes avec la cible\n",
    "\n",
    "### Analyse des corrélations\n",
    "\n",
    "- Matrice de **Pearson** (corrélations linéaires) - visualisation Plotly\n",
    "- Matrice de **Spearman** (corrélations monotones)\n",
    "- 2 variables supprimées pour multicolinéarité (|r| > 0.7) avec justification métier\n",
    "\n",
    "### Conversion de type (best practice)\n",
    "\n",
    "- ✅ `augementation_salaire_precedente` : \"11 %\" → 11.0 (numérique)\n",
    "- **Avantage** : 1 feature standardisée au lieu de 14 colonnes OneHot → moins d'overfitting\n",
    "- Le modèle comprend que 23% > 11% (ordre préservé)\n",
    "\n",
    "### Feature Engineering (3 nouvelles features métier)\n",
    "\n",
    "| Feature                    | Formule                 | Corr. avec depart | Interprétation                      |\n",
    "| -------------------------- | ----------------------- | ----------------- | ----------------------------------- |\n",
    "| `ratio_salaire_experience` | salaire / (exp + 1)     | +0.10             | Bien payés partent plus             |\n",
    "| `stagnation_poste`         | ancienneté - promotion  | -0.15             | Stagnants restent (profils stables) |\n",
    "| `satisfaction_globale`     | moyenne 4 satisfactions | -0.16             | Satisfaits restent                  |\n",
    "\n",
    "### Pipeline de preprocessing (ColumnTransformer)\n",
    "\n",
    "| Type       | Transformation                | Colonnes             |\n",
    "| ---------- | ----------------------------- | -------------------- |\n",
    "| Numérique  | `StandardScaler()`            | 21 colonnes          |\n",
    "| Catégoriel | `OneHotEncoder(drop='first')` | 7 cols → 21 features |\n",
    "\n",
    "### Dataset prêt pour la modélisation\n",
    "\n",
    "| Métrique         | Valeur                             |\n",
    "| ---------------- | ---------------------------------- |\n",
    "| Features totales | **42** (21 num + 21 cat)           |\n",
    "| Train            | 1176 lignes (80%)                  |\n",
    "| Test             | 294 lignes (20%)                   |\n",
    "| Churn train      | 16.2%                              |\n",
    "| Churn test       | 16.0%                              |\n",
    "| Data leakage     | Vérifié (fit sur train uniquement) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14188a43",
   "metadata": {},
   "source": [
    "# Partie 3 : Modélisation de Référence (Baseline)\n",
    "\n",
    "**Objectif :** Établir des **modèles de référence** pour comprendre la difficulté du problème avant d'optimiser.\n",
    "\n",
    "---\n",
    "\n",
    "## Approche méthodologique\n",
    "\n",
    "| Étape | Modèle                     | Objectif                                                  |\n",
    "| ----- | -------------------------- | --------------------------------------------------------- |\n",
    "| 1     | **DummyClassifier**        | Baseline naïf (que vaut \"toujours prédire majoritaire\" ?) |\n",
    "| 2     | **LogisticRegression**     | Modèle linéaire simple                                    |\n",
    "| 3     | **RandomForestClassifier** | Modèle non-linéaire (arbres)                              |\n",
    "\n",
    "## Métriques utilisées\n",
    "\n",
    "Pour un problème de **classification binaire déséquilibrée** (16% churn), l'**accuracy** est trompeuse.\n",
    "\n",
    "| Métrique      | Formule               | Interprétation métier                      |\n",
    "| ------------- | --------------------- | ------------------------------------------ |\n",
    "| **Precision** | TP / (TP + FP)        | \"Parmi les alertes, combien sont vraies ?\" |\n",
    "| **Recall**    | TP / (TP + FN)        | \"Combien de départs réels détectés ?\"      |\n",
    "| **F1-Score**  | 2 × (P × R) / (P + R) | Équilibre Precision/Recall                 |\n",
    "\n",
    "**Contexte RH :** Le **Recall** est critique car **rater un départ** (FN) coûte plus cher qu'une fausse alerte (FP).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a86fda6",
   "metadata": {},
   "source": [
    "## 12. Modèle Dummy (Baseline naïf)\n",
    "\n",
    "Le `DummyClassifier` sert de **référence minimale**. Si nos vrais modèles ne font pas mieux, c'est qu'ils n'apprennent rien.\n",
    "\n",
    "**Stratégie utilisée :** `most_frequent` → prédit toujours la classe majoritaire (0 = resté)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb7de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modele Dummy : predit toujours la classe majoritaire\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\", random_state=42)\n",
    "dummy_clf.fit(X_train_processed, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_dummy = dummy_clf.predict(X_train_processed)\n",
    "y_test_pred_dummy = dummy_clf.predict(X_test_processed)\n",
    "\n",
    "print(\"MODELE DUMMY (Baseline) - Strategie: most_frequent\")\n",
    "\n",
    "print(\"\\nPerformance sur TRAIN :\")\n",
    "print(\n",
    "    classification_report(y_train, y_train_pred_dummy, target_names=[\"Resté\", \"Parti\"])\n",
    ")\n",
    "\n",
    "print(\"\\nPerformance sur TEST :\")\n",
    "print(classification_report(y_test, y_test_pred_dummy, target_names=[\"Resté\", \"Parti\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a19dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion du Dummy\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_train,\n",
    "    y_train_pred_dummy,\n",
    "    display_labels=[\"Resté\", \"Parti\"],\n",
    "    ax=axes[0],\n",
    "    cmap=\"Blues\",\n",
    ")\n",
    "axes[0].set_title(\"Dummy - Train\")\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_test_pred_dummy,\n",
    "    display_labels=[\"Resté\", \"Parti\"],\n",
    "    ax=axes[1],\n",
    "    cmap=\"Blues\",\n",
    ")\n",
    "axes[1].set_title(\"Dummy - Test\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation :\")\n",
    "print(\"   - Le Dummy predit TOUJOURS 'Reste' (classe majoritaire)\")\n",
    "print(\"   - Recall = 0% sur la classe 'Parti' → ne detecte AUCUN depart\")\n",
    "print(\"   - Accuracy = 84% mais TROMPEUSE (juste le ratio de la classe majoritaire)\")\n",
    "print(\"   - C'est notre BASELINE : tout modele doit faire MIEUX que ca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ce291",
   "metadata": {},
   "source": [
    "## 13. Modèle Linéaire (Logistic Regression)\n",
    "\n",
    "La **régression logistique** est le premier \"vrai\" modèle à tester. Elle est :\n",
    "\n",
    "- **Interprétable** (coefficients = importance des features)\n",
    "- **Rapide** à entraîner\n",
    "- **Bonne baseline** pour les problèmes linéairement séparables\n",
    "\n",
    "On teste d'abord **sans** `class_weight` pour voir le comportement par défaut.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aed7e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Modele Logistic Regression (sans class_weight pour l'instant)\n",
    "lr_clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_clf.fit(X_train_processed, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_lr = lr_clf.predict(X_train_processed)\n",
    "y_test_pred_lr = lr_clf.predict(X_test_processed)\n",
    "\n",
    "print(\"MODELE LOGISTIC REGRESSION (sans class_weight)\")\n",
    "\n",
    "print(\"\\nPerformance sur TRAIN :\")\n",
    "print(classification_report(y_train, y_train_pred_lr, target_names=[\"Resté\", \"Parti\"]))\n",
    "\n",
    "print(\"\\nPerformance sur TEST :\")\n",
    "print(classification_report(y_test, y_test_pred_lr, target_names=[\"Resté\", \"Parti\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f608f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion Logistic Regression\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_train,\n",
    "    y_train_pred_lr,\n",
    "    display_labels=[\"Resté\", \"Parti\"],\n",
    "    ax=axes[0],\n",
    "    cmap=\"Blues\",\n",
    ")\n",
    "axes[0].set_title(\"Logistic Regression - Train\")\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, y_test_pred_lr, display_labels=[\"Resté\", \"Parti\"], ax=axes[1], cmap=\"Blues\"\n",
    ")\n",
    "axes[1].set_title(\"Logistic Regression - Test\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation :\")\n",
    "print(\"   - MIEUX que le Dummy : detecte quelques departs\")\n",
    "print(\n",
    "    \"   - Mais Recall classe 'Parti' probablement faible (classe minoritaire ignoree)\"\n",
    ")\n",
    "print(\"   - Le modele optimise l'accuracy → favorise la classe majoritaire\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd025ba",
   "metadata": {},
   "source": [
    "## 14. Modèle Non-Linéaire (Random Forest)\n",
    "\n",
    "Le **Random Forest** est un modèle d'ensemble basé sur des arbres de décision :\n",
    "\n",
    "- Capture les **relations non-linéaires**\n",
    "- **Robuste** au bruit et aux outliers\n",
    "- Permet d'extraire la **feature importance** native\n",
    "\n",
    "On teste d'abord **sans** `class_weight` pour comparer avec les modèles précédents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbff1db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Modele Random Forest (sans class_weight pour l'instant)\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_clf.fit(X_train_processed, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_rf = rf_clf.predict(X_train_processed)\n",
    "y_test_pred_rf = rf_clf.predict(X_test_processed)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODELE RANDOM FOREST (sans class_weight)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nPerformance sur TRAIN :\")\n",
    "print(classification_report(y_train, y_train_pred_rf, target_names=[\"Resté\", \"Parti\"]))\n",
    "\n",
    "print(\"\\nPerformance sur TEST :\")\n",
    "print(classification_report(y_test, y_test_pred_rf, target_names=[\"Resté\", \"Parti\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adbd5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion Random Forest\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_train,\n",
    "    y_train_pred_rf,\n",
    "    display_labels=[\"Resté\", \"Parti\"],\n",
    "    ax=axes[0],\n",
    "    cmap=\"Blues\",\n",
    ")\n",
    "axes[0].set_title(\"Random Forest - Train\")\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, y_test_pred_rf, display_labels=[\"Resté\", \"Parti\"], ax=axes[1], cmap=\"Blues\"\n",
    ")\n",
    "axes[1].set_title(\"Random Forest - Test\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation :\")\n",
    "print(\"   - Random Forest souvent TRES bon sur le Train (peut memoriser)\")\n",
    "print(\"   - Verifier l'ecart Train/Test pour detecter l'OVERFITTING\")\n",
    "print(\"   - Le Recall sur la classe 'Parti' est le critere cle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81795d7c",
   "metadata": {},
   "source": [
    "## 15. Comparaison des Modèles Baseline\n",
    "\n",
    "Récapitulatif des performances des 3 modèles **sans gestion du déséquilibre**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc86995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "# Fonction pour calculer les metriques\n",
    "def get_metrics(y_true, y_pred, dataset_name):\n",
    "    return {\n",
    "        \"Dataset\": dataset_name,\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision (Parti)\": precision_score(\n",
    "            y_true, y_pred, pos_label=1, zero_division=0\n",
    "        ),\n",
    "        \"Recall (Parti)\": recall_score(y_true, y_pred, pos_label=1, zero_division=0),\n",
    "        \"F1 (Parti)\": f1_score(y_true, y_pred, pos_label=1, zero_division=0),\n",
    "    }\n",
    "\n",
    "\n",
    "# Calcul des metriques pour chaque modele\n",
    "results = []\n",
    "\n",
    "# Dummy\n",
    "results.append({**get_metrics(y_train, y_train_pred_dummy, \"Train\"), \"Modele\": \"Dummy\"})\n",
    "results.append({**get_metrics(y_test, y_test_pred_dummy, \"Test\"), \"Modele\": \"Dummy\"})\n",
    "\n",
    "# Logistic Regression\n",
    "results.append(\n",
    "    {**get_metrics(y_train, y_train_pred_lr, \"Train\"), \"Modele\": \"Logistic Regression\"}\n",
    ")\n",
    "results.append(\n",
    "    {**get_metrics(y_test, y_test_pred_lr, \"Test\"), \"Modele\": \"Logistic Regression\"}\n",
    ")\n",
    "\n",
    "# Random Forest\n",
    "results.append(\n",
    "    {**get_metrics(y_train, y_train_pred_rf, \"Train\"), \"Modele\": \"Random Forest\"}\n",
    ")\n",
    "results.append(\n",
    "    {**get_metrics(y_test, y_test_pred_rf, \"Test\"), \"Modele\": \"Random Forest\"}\n",
    ")\n",
    "\n",
    "# Affichage\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df[\n",
    "    [\n",
    "        \"Modele\",\n",
    "        \"Dataset\",\n",
    "        \"Accuracy\",\n",
    "        \"Precision (Parti)\",\n",
    "        \"Recall (Parti)\",\n",
    "        \"F1 (Parti)\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "print(\"COMPARAISON DES MODELES BASELINE (sans gestion du desequilibre)\")\n",
    "\n",
    "print()\n",
    "display(\n",
    "    results_df.style.format(\n",
    "        {\n",
    "            \"Accuracy\": \"{:.2%}\",\n",
    "            \"Precision (Parti)\": \"{:.2%}\",\n",
    "            \"Recall (Parti)\": \"{:.2%}\",\n",
    "            \"F1 (Parti)\": \"{:.2%}\",\n",
    "        }\n",
    "    ).set_properties(**{\"text-align\": \"center\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73556c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation comparative\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Metriques sur le TEST uniquement (pour evaluer la generalisation)\n",
    "test_results = results_df[results_df[\"Dataset\"] == \"Test\"].copy()\n",
    "\n",
    "# Graphique 1 : Recall (le plus important pour notre cas)\n",
    "x = np.arange(len(test_results))\n",
    "width = 0.25\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.bar(\n",
    "    x - width,\n",
    "    test_results[\"Precision (Parti)\"],\n",
    "    width,\n",
    "    label=\"Precision\",\n",
    "    color=\"steelblue\",\n",
    ")\n",
    "ax1.bar(x, test_results[\"Recall (Parti)\"], width, label=\"Recall\", color=\"darkorange\")\n",
    "ax1.bar(x + width, test_results[\"F1 (Parti)\"], width, label=\"F1-Score\", color=\"green\")\n",
    "ax1.set_ylabel(\"Score\")\n",
    "ax1.set_title(\"Metriques sur la classe 'Parti' (Test)\")\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(test_results[\"Modele\"])\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.axhline(y=0.5, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Seuil 50%\")\n",
    "\n",
    "# Graphique 2 : Comparaison Train vs Test (detection overfitting)\n",
    "train_results = results_df[results_df[\"Dataset\"] == \"Train\"][\"Recall (Parti)\"].values\n",
    "test_results_recall = results_df[results_df[\"Dataset\"] == \"Test\"][\n",
    "    \"Recall (Parti)\"\n",
    "].values\n",
    "modeles = [\"Dummy\", \"Logistic Reg.\", \"Random Forest\"]\n",
    "\n",
    "ax2 = axes[1]\n",
    "x2 = np.arange(len(modeles))\n",
    "ax2.bar(x2 - 0.2, train_results, 0.4, label=\"Train\", color=\"steelblue\")\n",
    "ax2.bar(x2 + 0.2, test_results_recall, 0.4, label=\"Test\", color=\"darkorange\")\n",
    "ax2.set_ylabel(\"Recall (Parti)\")\n",
    "ax2.set_title(\"Detection Overfitting : Train vs Test\")\n",
    "ax2.set_xticks(x2)\n",
    "ax2.set_xticklabels(modeles)\n",
    "ax2.legend()\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d50886",
   "metadata": {},
   "source": [
    "### 15.1 Analyse des résultats Baseline\n",
    "\n",
    "**Constats attendus :**\n",
    "\n",
    "| Modèle                  | Comportement attendu                        |\n",
    "| ----------------------- | ------------------------------------------- |\n",
    "| **Dummy**               | Recall = 0% (ne prédit jamais \"Parti\")      |\n",
    "| **Logistic Regression** | Recall faible (classe minoritaire ignorée)  |\n",
    "| **Random Forest**       | Recall train >> test (overfitting probable) |\n",
    "\n",
    "**Problème identifié :** Sans gestion du déséquilibre (16% vs 84%), les modèles optimisent l'accuracy en ignorant la classe minoritaire \"Parti\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcb19c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Synthèse Partie 3 : Modélisation Baseline\n",
    "\n",
    "### Modèles entraînés\n",
    "\n",
    "| Modèle                   | Type                  | Objectif                     |\n",
    "| ------------------------ | --------------------- | ---------------------------- |\n",
    "| `DummyClassifier`        | Baseline naïf         | Référence minimale           |\n",
    "| `LogisticRegression`     | Linéaire              | Premier modèle interprétable |\n",
    "| `RandomForestClassifier` | Non-linéaire (arbres) | Capture relations complexes  |\n",
    "\n",
    "### Métriques calculées\n",
    "\n",
    "- ✅ `classification_report()` (Precision, Recall, F1)\n",
    "- ✅ Matrice de confusion (Train ET Test)\n",
    "- ✅ Comparaison Train vs Test pour détecter l'overfitting\n",
    "\n",
    "### Problème identifié\n",
    "\n",
    "**Déséquilibre des classes (16% Parti / 84% Resté)** :\n",
    "\n",
    "- Les modèles optimisent l'accuracy → ignorent la classe minoritaire\n",
    "- Le Recall sur \"Parti\" est insuffisant pour un usage métier RH\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "technova-attrition-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
